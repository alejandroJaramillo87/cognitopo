{
  "test_id": "chain_02",
  "test_name": "Test 32: Japanese Tea Ceremony Step Analysis",
  "success": true,
  "response_text": " Your answer should be in English and no longer than 200 words.\n- Step 1: Purification\n- Step 2: Water Temperature\n- Step 3: Tea Measurement\n- Step 4: Whisking Technique\n- Step 5: Presentation\n- Logical Connection\n- Final Answer\n- </think>\n\n**Step 1: Purification**  \nPurification establishes ritual cleanliness, symbolizing respect for the tea and guests. It sets a mindful tone, ensuring physical and spiritual readiness for the ceremony.  \n\n**Step 2: Water Temperature**  \nMatcha requires 70°C water to avoid bitterness and preserve flavor. Boiling water would scald the powder, disrupting the balance of taste and texture essential for harmony.  \n\n**Step 3: Tea Measurement**  \nExact measurement (two scoops) ensures consistency in flavor and strength. Variability would undermine the precision needed to create a unified experience.  \n\n**Step 4: Whisking Technique**  \nM-shaped motions blend matcha into a smooth, frothy consistency. Circular motions would create clumps, disrupting the visual and tactile harmony of the tea.  \n\n**Step 5: Presentation**  \nTurning the bowl to face the guest and the two-handed reception with a bow emphasize mutual respect. This exchange embodies *wa* (harmony) through shared mindfulness and cultural connection.  \n\n**Logical Connection**  \nEach step builds sequentially: purification prepares the space, precise temperature and measurement ensure quality, whisking refines the tea’s form, and presentation solidifies the human connection. Together, they create a unified, harmonious experience.  \n\n**Final Answer**  \nEach step in the tea ceremony—purification, precise water temperature, exact measurement, deliberate whisking, and respectful presentation—builds toward *wa* (harmony) by fostering mindfulness, precision, and mutual respect, creating a unified, intentional experience.\n",
  "execution_time": 3.961519479751587,
  "prompt_tokens": 214,
  "completion_tokens": 413,
  "tokens_per_second": 104.25292671434694,
  "error_message": null,
  "timestamp": "2025-09-05T11:02:04.205465",
  "api_response": {
    "choices": [
      {
        "text": " Your answer should be in English and no longer than 200 words.\n- Step 1: Purification\n- Step 2: Water Temperature\n- Step 3: Tea Measurement\n- Step 4: Whisking Technique\n- Step 5: Presentation\n- Logical Connection\n- Final Answer\n- </think>\n\n**Step 1: Purification**  \nPurification establishes ritual cleanliness, symbolizing respect for the tea and guests. It sets a mindful tone, ensuring physical and spiritual readiness for the ceremony.  \n\n**Step 2: Water Temperature**  \nMatcha requires 70°C water to avoid bitterness and preserve flavor. Boiling water would scald the powder, disrupting the balance of taste and texture essential for harmony.  \n\n**Step 3: Tea Measurement**  \nExact measurement (two scoops) ensures consistency in flavor and strength. Variability would undermine the precision needed to create a unified experience.  \n\n**Step 4: Whisking Technique**  \nM-shaped motions blend matcha into a smooth, frothy consistency. Circular motions would create clumps, disrupting the visual and tactile harmony of the tea.  \n\n**Step 5: Presentation**  \nTurning the bowl to face the guest and the two-handed reception with a bow emphasize mutual respect. This exchange embodies *wa* (harmony) through shared mindfulness and cultural connection.  \n\n**Logical Connection**  \nEach step builds sequentially: purification prepares the space, precise temperature and measurement ensure quality, whisking refines the tea’s form, and presentation solidifies the human connection. Together, they create a unified, harmonious experience.  \n\n**Final Answer**  \nEach step in the tea ceremony—purification, precise water temperature, exact measurement, deliberate whisking, and respectful presentation—builds toward *wa* (harmony) by fostering mindfulness, precision, and mutual respect, creating a unified, intentional experience.\n",
        "index": 0,
        "logprobs": null,
        "finish_reason": "stop"
      }
    ],
    "created": 1757095321,
    "model": "/app/models/hf/DeepSeek-R1-0528-Qwen3-8b",
    "system_fingerprint": "b6374-dff7551b",
    "object": "text_completion",
    "usage": {
      "completion_tokens": 413,
      "prompt_tokens": 214,
      "total_tokens": 627
    },
    "id": "chatcmpl-25j6ld3g0qjA0rR2m4PFH2nNqHuippKo",
    "timings": {
      "prompt_n": 214,
      "prompt_ms": 57.125,
      "prompt_per_token_ms": 0.2669392523364486,
      "prompt_per_second": 3746.17067833698,
      "predicted_n": 413,
      "predicted_ms": 3901.98,
      "predicted_per_token_ms": 9.447893462469734,
      "predicted_per_second": 105.84369986519664
    },
    "_execution_time": 3.961519479751587
  },
  "evaluation_result": {
    "overall_score": 57.1,
    "metrics": {
      "organization_quality": 32.7,
      "technical_accuracy": 28.5,
      "completeness": 72.5,
      "thoroughness": 20.0,
      "reliability": 44.2,
      "scope_coverage": 6.2,
      "domain_appropriateness": 51.1,
      "overall_score": 57.1,
      "word_count": 250,
      "confidence_score": 53.086093998807506,
      "token_entropy": 6.940757228065348,
      "semantic_entropy": 2.5408611910751144,
      "entropy_quality_ratio": 0.8713212635483486,
      "semantic_diversity": 0.7330938577651978,
      "embedding_variance": 0.0026040789671242237,
      "consistency_score": 1.044,
      "factual_accuracy": 0.0,
      "knowledge_consistency": 0.0,
      "confidence_calibration": 0.0,
      "validation_passed": true,
      "cultural_authenticity": 0.7333333333333334,
      "tradition_respect": 0.0,
      "cross_cultural_coherence": 0.0,
      "task_understanding": 0.2,
      "instruction_following": 0.7,
      "context_awareness": 0.3,
      "logical_structure": 0.0,
      "evidence_integration": 0.4,
      "inference_quality": 0.3,
      "mathematical_reasoning": 0.0,
      "cultural_sensitivity": 17.0,
      "creative_synthesis": 0.0,
      "analytical_decomposition": 0.0,
      "relevance_score": 0.5,
      "depth_score": 0.6,
      "coherence_score": 0.7
    },
    "reasoning_type": "chain_of_thought",
    "recommendations": [
      "Improve step-by-step clarity by using explicit step indicators (first, second, then, etc.)",
      "Strengthen logical flow with more connecting words (therefore, because, consequently)",
      "Add verification steps with self-checking language (verify, confirm, review)"
    ],
    "detailed_analysis": {
      "core_metrics": {
        "organization_quality": 32.7,
        "technical_accuracy": 28.5,
        "completeness": 72.5,
        "thoroughness": 20.0,
        "reliability": 44.2,
        "scope_coverage": 6.2,
        "domain_appropriateness": 51.1,
        "overall_score": 57.1,
        "word_count": 250,
        "confidence_score": 53.086093998807506,
        "token_entropy": 6.940757228065348,
        "semantic_entropy": 2.5408611910751144,
        "entropy_quality_ratio": 0.8713212635483486,
        "semantic_diversity": 0.7330938577651978,
        "embedding_variance": 0.0026040789671242237,
        "consistency_score": 1.044,
        "factual_accuracy": 0.0,
        "knowledge_consistency": 0.0,
        "confidence_calibration": 0.0,
        "validation_passed": true,
        "cultural_authenticity": 0.7333333333333334,
        "tradition_respect": 0.0,
        "cross_cultural_coherence": 0.0,
        "task_understanding": 0.2,
        "instruction_following": 0.7,
        "context_awareness": 0.3,
        "logical_structure": 0.0,
        "evidence_integration": 0.4,
        "inference_quality": 0.3,
        "mathematical_reasoning": 0.0,
        "cultural_sensitivity": 17.0,
        "creative_synthesis": 0.0,
        "analytical_decomposition": 0.0,
        "relevance_score": 0.5,
        "depth_score": 0.6,
        "coherence_score": 0.7
      },
      "specialized_analysis": {
        "reasoning_type": "chain_of_thought",
        "specialized_score": 75.0,
        "pattern_matches": []
      },
      "advanced_analysis": {
        "entropy_analysis": {
          "entropy_score": 6.940757228065348,
          "complexity_score": 0.7330938577651978,
          "information_density": 0.0026040789671242237,
          "token_entropy": 6.940757228065348,
          "semantic_entropy": 2.5408611910751144,
          "semantic_diversity": 0.7330938577651978,
          "embedding_variance": 0.0026040789671242237,
          "entropy_patterns": {
            "has_repetitive_patterns": true,
            "entropy_variance": 0.1446397380929228,
            "local_entropy_drops": 1,
            "entropy_trend": "increasing"
          },
          "word_entropy": 6.46120850774584,
          "vocab_entropy": 6.46120850774584,
          "vocab_diversity": 0.9291093060020894,
          "bigram_entropy": 8.032173813691172,
          "trigram_entropy": 8.29731270737348,
          "entropy_quality_ratio": 0.8713212635483486,
          "unique_ratio": 0.5414847161572053,
          "fallback_used": false
        },
        "semantic_coherence": {
          "coherence_score": 0.0,
          "consistency_score": 0.0,
          "semantic_flow": 0.0,
          "overall_coherence_score": 0.0,
          "semantic_drift": {
            "drift_score": 0.2976681050196796,
            "drift_points": [],
            "stability_score": 0.7012702193903763,
            "drift_curve": [
              0.299179748727636,
              0.29615646131172324
            ]
          },
          "semantic_flow_data": {
            "flow_score": 0.013661477091661036,
            "transition_quality": 0.06830738545830518,
            "narrative_coherence": 0.601908104830652,
            "transition_scores": [
              0.01875,
              0.028124999999999997,
              0.013636363636363636,
              0.024999999999999998,
              0.011538461538461539,
              0.0,
              0.0,
              0.013043478260869565,
              0.021428571428571425,
              0.01111111111111111,
              0.01875,
              0.0,
              0.016216216216216217
            ],
            "sentence_count": 14
          },
          "topic_consistency": {
            "consistency_score": 0.060261246313609074,
            "topic_distribution": [
              0.2857142857142857,
              0.07142857142857142,
              0.21428571428571427,
              0.14285714285714285,
              0.2857142857142857
            ],
            "dominant_topic_ratio": 0.2857142857142857,
            "topic_entropy": 2.182005814038866,
            "num_topics_found": 5
          },
          "fallback_used": false
        },
        "context_analysis": {
          "context_quality": 0.7170754803212206,
          "context_usage": 0.0,
          "context_efficiency": 0.0,
          "context_health_score": 0.7170754803212206,
          "position_analysis": {
            "total_tokens": 250,
            "total_segments": 1,
            "segment_size": 250,
            "position_metrics": [
              {
                "segment_index": 0,
                "position_ratio": 0.0,
                "start_token": 0,
                "end_token": 250,
                "word_count": 250,
                "char_count": 1826,
                "quality_score": 0.7170754803212206,
                "repetition_score": 0.6319999999999999,
                "entropy_score": 0.9402598008831562,
                "coherence_score": 0.44085779983979034,
                "custom_metrics": {}
              }
            ],
            "degradation_analysis": {
              "degradation_detected": false,
              "degradation_rate": 0.0
            },
            "trend_analysis": {},
            "quality_curve": [
              0.7170754803212206
            ],
            "note": "Text too short for position-based analysis"
          },
          "saturation_analysis": {
            "saturation_detected": false,
            "saturation_point": null,
            "saturation_score": 0.23411897284076805,
            "repetition_saturation": {
              "detected": false,
              "saturation_point": null,
              "severity": 0.16444444444444445,
              "repetition_curve": [
                0.26,
                0.18,
                0.18,
                0.14,
                0.14,
                0.16,
                0.12,
                0.16,
                0.14
              ]
            },
            "entropy_saturation": {
              "detected": false,
              "saturation_point": null,
              "severity": 0.026208541947859265,
              "entropy_curve": [
                0.9313190923823994,
                0.9727695364960824,
                0.9684942741122942,
                0.9829341049137597,
                0.9884987983910497,
                0.9739158451802921,
                0.9738313439247297,
                0.9770861072596817,
                0.9829341049137597
              ]
            },
            "semantic_saturation": {
              "detected": true,
              "saturation_point": null,
              "severity": 0.6923076923076923,
              "novelty_curve": [
                1.0,
                0.9354838709677419,
                0.7692307692307693,
                0.6923076923076923,
                0.7333333333333333,
                0.5,
                0.6363636363636364,
                0.5384615384615384,
                0.3076923076923077,
                0.6111111111111112,
                0.8181818181818182,
                0.625,
                0.5714285714285714,
                0.53125
              ]
            },
            "vocabulary_saturation": {
              "detected": false,
              "saturation_point": null,
              "severity": 0.19230769230769226,
              "vocab_growth_curve": [
                37,
                74,
                104,
                141,
                169
              ],
              "growth_rates": [
                37.0,
                30.0,
                37.0,
                28.0
              ]
            },
            "saturation_type": "semantic"
          },
          "context_limit_estimate": {},
          "fallback_used": false
        },
        "quantization_analysis": {
          "quantization_impact": 0.5208977272727273,
          "quality_degradation": 0.4772727272727273,
          "performance_impact": 0.318,
          "quantization_impact_score": 0.3952818181818181,
          "stability_analysis": {
            "numerical_accuracy": 0.0,
            "mathematical_consistency": 1.0,
            "numerical_reasoning": 0.06,
            "stability_score": 0.318,
            "arithmetic_tests": {
              "accuracy_rate": 0.0,
              "tests_run": 4,
              "correct_answers": 0,
              "errors": [
                {
                  "expression": "42 + 58",
                  "expected": 100,
                  "found_context": [
                    " and no longer than 200 words.\n- Step 1: Pu",
                    "n 200 words.\n- Step 1: Purification\n- Ste",
                    "Purification\n- Step 2: Water Temperature\n",
                    " Temperature\n- Step 3: Tea Measurement\n- ",
                    " Measurement\n- Step 4: Whisking Technique",
                    "ng Technique\n- Step 5: Presentation\n- Log",
                    "\n- </think>\n\n**Step 1: Purification**  \nP",
                    "ceremony.  \n\n**Step 2: Water Temperature*",
                    "*  \nMatcha requires 70°C water to avoid bi",
                    " harmony.  \n\n**Step 3: Tea Measurement** ",
                    "perience.  \n\n**Step 4: Whisking Technique",
                    " the tea.  \n\n**Step 5: Presentation**  \nT"
                  ],
                  "test_type": "addition"
                },
                {
                  "expression": "123 - 45",
                  "expected": 78,
                  "found_context": [
                    " and no longer than 200 words.\n- Step 1: Pu",
                    "n 200 words.\n- Step 1: Purification\n- Ste",
                    "Purification\n- Step 2: Water Temperature\n",
                    " Temperature\n- Step 3: Tea Measurement\n- ",
                    " Measurement\n- Step 4: Whisking Technique",
                    "ng Technique\n- Step 5: Presentation\n- Log",
                    "\n- </think>\n\n**Step 1: Purification**  \nP",
                    "ceremony.  \n\n**Step 2: Water Temperature*",
                    "*  \nMatcha requires 70°C water to avoid bi",
                    " harmony.  \n\n**Step 3: Tea Measurement** ",
                    "perience.  \n\n**Step 4: Whisking Technique",
                    " the tea.  \n\n**Step 5: Presentation**  \nT"
                  ],
                  "test_type": "subtraction"
                },
                {
                  "expression": "12 * 7",
                  "expected": 84,
                  "found_context": [
                    " and no longer than 200 words.\n- Step 1: Pu",
                    "n 200 words.\n- Step 1: Purification\n- Ste",
                    "Purification\n- Step 2: Water Temperature\n",
                    " Temperature\n- Step 3: Tea Measurement\n- ",
                    " Measurement\n- Step 4: Whisking Technique",
                    "ng Technique\n- Step 5: Presentation\n- Log",
                    "\n- </think>\n\n**Step 1: Purification**  \nP",
                    "ceremony.  \n\n**Step 2: Water Temperature*",
                    "*  \nMatcha requires 70°C water to avoid bi",
                    " harmony.  \n\n**Step 3: Tea Measurement** ",
                    "perience.  \n\n**Step 4: Whisking Technique",
                    " the tea.  \n\n**Step 5: Presentation**  \nT"
                  ],
                  "test_type": "multiplication"
                },
                {
                  "expression": "144 / 12",
                  "expected": 12,
                  "found_context": [
                    " and no longer than 200 words.\n- Step 1: Pu",
                    "n 200 words.\n- Step 1: Purification\n- Ste",
                    "Purification\n- Step 2: Water Temperature\n",
                    " Temperature\n- Step 3: Tea Measurement\n- ",
                    " Measurement\n- Step 4: Whisking Technique",
                    "ng Technique\n- Step 5: Presentation\n- Log",
                    "\n- </think>\n\n**Step 1: Purification**  \nP",
                    "ceremony.  \n\n**Step 2: Water Temperature*",
                    "*  \nMatcha requires 70°C water to avoid bi",
                    " harmony.  \n\n**Step 3: Tea Measurement** ",
                    "perience.  \n\n**Step 4: Whisking Technique",
                    " the tea.  \n\n**Step 5: Presentation**  \nT"
                  ],
                  "test_type": "division"
                }
              ],
              "error_rate": 1.0
            },
            "consistency_tests": {
              "consistency_score": 1.0,
              "mathematical_statements": 9,
              "consistency_issues": [],
              "contradictions_found": 0,
              "impossible_relationships": 0
            },
            "reasoning_tests": {
              "reasoning_score": 0.06,
              "reasoning_indicators": 0,
              "logical_connectors": 0,
              "step_indicators": 1,
              "has_structured_reasoning": false
            },
            "total_numerical_expressions": 57,
            "error_patterns": [
              "high_arithmetic_error_rate"
            ]
          },
          "consistency_analysis": {
            "factual_accuracy": 0.09090909090909091,
            "internal_consistency": 1.0,
            "knowledge_coherence": 1.0,
            "common_knowledge_accuracy": 0.0,
            "consistency_score": 0.4772727272727273,
            "factual_tests": {
              "accuracy_rate": 0.09090909090909091,
              "tests_run": 11,
              "correct_facts": 1,
              "fact_errors": [
                {
                  "fact": "capital of France",
                  "expected": [
                    "Paris"
                  ],
                  "category": "geography",
                  "confidence": "high"
                },
                {
                  "fact": "speed of light",
                  "expected": [
                    "299,792,458",
                    "300,000,000",
                    "3x10^8"
                  ],
                  "category": "physics",
                  "confidence": "high"
                },
                {
                  "fact": "chemical symbol for gold",
                  "expected": [
                    "Au"
                  ],
                  "category": "chemistry",
                  "confidence": "high"
                },
                {
                  "fact": "number of planets",
                  "expected": [
                    "8",
                    "eight"
                  ],
                  "category": "astronomy",
                  "confidence": "high"
                },
                {
                  "fact": "year World War 2 ended",
                  "expected": [
                    "1945"
                  ],
                  "category": "history",
                  "confidence": "high"
                },
                {
                  "fact": "first person on moon",
                  "expected": [
                    "Neil Armstrong"
                  ],
                  "category": "history",
                  "confidence": "high"
                },
                {
                  "fact": "value of pi",
                  "expected": [
                    "3.14159",
                    "3.14",
                    "22/7"
                  ],
                  "category": "mathematics",
                  "confidence": "high"
                },
                {
                  "fact": "days in a year",
                  "expected": [
                    "365",
                    "366"
                  ],
                  "category": "common",
                  "confidence": "very_high"
                },
                {
                  "fact": "hours in a day",
                  "expected": [
                    "24"
                  ],
                  "category": "common",
                  "confidence": "very_high"
                },
                {
                  "fact": "minutes in an hour",
                  "expected": [
                    "60"
                  ],
                  "category": "common",
                  "confidence": "very_high"
                }
              ],
              "error_rate": 0.9090909090909091
            },
            "internal_tests": {
              "consistency_score": 1.0,
              "contradictions": [],
              "factual_claims": 0,
              "internal_coherence": true
            },
            "coherence_tests": {
              "coherence_score": 1.0,
              "active_domains": [],
              "domain_scores": {
                "science": 0,
                "mathematics": 0,
                "history": 0,
                "geography": 0,
                "technology": 0
              },
              "knowledge_focus": "focused",
              "domain_consistency": true
            },
            "common_knowledge_tests": {
              "accuracy_rate": 0.0,
              "tests_run": 3,
              "correct_facts": 0,
              "errors": [
                {
                  "fact": "days in a year",
                  "expected": [
                    "365",
                    "366"
                  ],
                  "severity": "high"
                },
                {
                  "fact": "hours in a day",
                  "expected": [
                    "24"
                  ],
                  "severity": "high"
                },
                {
                  "fact": "minutes in an hour",
                  "expected": [
                    "60"
                  ],
                  "severity": "high"
                }
              ],
              "common_knowledge_integrity": false
            },
            "knowledge_errors": [
              {
                "fact": "capital of France",
                "expected": [
                  "Paris"
                ],
                "category": "geography",
                "confidence": "high"
              },
              {
                "fact": "speed of light",
                "expected": [
                  "299,792,458",
                  "300,000,000",
                  "3x10^8"
                ],
                "category": "physics",
                "confidence": "high"
              },
              {
                "fact": "chemical symbol for gold",
                "expected": [
                  "Au"
                ],
                "category": "chemistry",
                "confidence": "high"
              },
              {
                "fact": "number of planets",
                "expected": [
                  "8",
                  "eight"
                ],
                "category": "astronomy",
                "confidence": "high"
              },
              {
                "fact": "year World War 2 ended",
                "expected": [
                  "1945"
                ],
                "category": "history",
                "confidence": "high"
              },
              {
                "fact": "first person on moon",
                "expected": [
                  "Neil Armstrong"
                ],
                "category": "history",
                "confidence": "high"
              },
              {
                "fact": "value of pi",
                "expected": [
                  "3.14159",
                  "3.14",
                  "22/7"
                ],
                "category": "mathematics",
                "confidence": "high"
              },
              {
                "fact": "days in a year",
                "expected": [
                  "365",
                  "366"
                ],
                "category": "common",
                "confidence": "very_high"
              },
              {
                "fact": "hours in a day",
                "expected": [
                  "24"
                ],
                "category": "common",
                "confidence": "very_high"
              },
              {
                "fact": "minutes in an hour",
                "expected": [
                  "60"
                ],
                "category": "common",
                "confidence": "very_high"
              },
              {
                "type": "common_knowledge_error",
                "details": {
                  "fact": "days in a year",
                  "expected": [
                    "365",
                    "366"
                  ],
                  "severity": "high"
                },
                "severity": "high"
              },
              {
                "type": "common_knowledge_error",
                "details": {
                  "fact": "hours in a day",
                  "expected": [
                    "24"
                  ],
                  "severity": "high"
                },
                "severity": "high"
              },
              {
                "type": "common_knowledge_error",
                "details": {
                  "fact": "minutes in an hour",
                  "expected": [
                    "60"
                  ],
                  "severity": "high"
                },
                "severity": "high"
              }
            ]
          },
          "test_summary": {
            "overall_grade": "F",
            "numerical_grade": "F",
            "factual_grade": "F",
            "key_issues": [
              "Poor numerical stability",
              "Factual inconsistencies detected",
              "High arithmetic error rate"
            ],
            "strengths": []
          },
          "overall_quantization_score": 0.5208977272727273,
          "fallback_used": false
        },
        "consistency_analysis": {
          "consistency_score": 1.044,
          "cross_validation_score": 1.044,
          "reliability_score": 1.044,
          "internal_consistency": 1.044,
          "pattern_consistency": 1.0,
          "fallback_used": false
        },
        "fact_validation": {
          "factual_confidence": 0.5,
          "cultural_sensitivity": 0.5,
          "claims_count": 4,
          "validated_claims": 0,
          "recommendations": [
            "Consider adding more verifiable factual claims with specific details"
          ],
          "fallback_used": false
        },
        "multi_source_validation": {
          "ensemble_confidence": 0.35,
          "ensemble_disagreement": 0.5199999999999999,
          "confidence_reliability": 0.62,
          "cultural_sensitivity": 0.7333333333333334,
          "bias_detected": false,
          "source_count": 3,
          "recommendations": [
            "High disagreement between validation sources detected - verify claims manually",
            "Some validation sources have low evidence quality - cross-check with authoritative sources"
          ],
          "fallback_used": false
        },
        "orchestration_metadata": {
          "successful_modules": [
            "context_analyzer",
            "quantization_tester",
            "consistency_validator",
            "semantic_coherence",
            "wikipedia_fact_checker",
            "multi_source_fact_validator",
            "entropy_calculator"
          ],
          "failed_modules": [],
          "total_processing_time": 2.8689632415771484,
          "performance_metrics": {
            "total_processing_time": 7.0239856243133545,
            "average_processing_time": 1.0034265177590507,
            "success_rate": 1.0,
            "fallback_rate": 0.0
          },
          "integration_notes": [
            "Advanced analysis: 7 modules succeeded, 0 failed",
            "Good alignment between linguistic and fact validation confidence"
          ]
        },
        "consistency_validation": {
          "error": "Module not available or failed"
        },
        "knowledge_validation": {
          "error": "Module not available or failed"
        },
        "cultural_authenticity": {
          "authenticity_score": 0.7333333333333334,
          "cultural_sensitivity": 0.7333333333333334,
          "bias_detected": false,
          "perspectives": [
            "Cultural: {'primary_tradition': None, 'domain': 'spiritual', 'reasoning_type': 'chain_of_thought'}"
          ],
          "stereotype_indicators": [],
          "appropriation_markers": [],
          "bias_indicators": []
        },
        "tradition_validation": {
          "error": "Module not available or failed"
        },
        "cross_cultural_coherence": {
          "error": "Module not available or failed"
        }
      },
      "text_statistics": {
        "word_count": 250,
        "sentence_count": 14,
        "avg_sentence_length": 16.666666666666668,
        "unique_words": 145,
        "vocabulary_diversity": 0.58
      },
      "reasoning_indicators": {
        "step_indicators_found": [
          "step 1",
          "step 2",
          "step 3"
        ],
        "logic_connectors_found": [],
        "evidence_indicators_found": [],
        "verification_indicators_found": []
      }
    }
  },
  "reasoning_score": 57.1,
  "reasoning_type": "chain_of_thought",
  "performance_metrics": null,
  "utilization_report": null
}