# Directory Structure Guide

This document explains the organization and purpose of every directory and key file in the benchmark test system. Think of this as the **project's file system architecture documentation**.

## ğŸ—ï¸ **Top-Level Organization**

```
benchmark_tests/
â”œâ”€â”€ ğŸ“ data/              # Reference data and datasets (like database fixtures)
â”œâ”€â”€ ğŸ“ domains/           # Test definitions (like API route definitions)  
â”œâ”€â”€ ğŸ“ evaluator/         # Business logic modules (like service layer)
â”œâ”€â”€ ğŸ“ tests/            # Framework tests (like typical test directories)
â”œâ”€â”€ ğŸ“ docs/             # Documentation (like any well-documented project)
â”œâ”€â”€ ğŸ“ scripts/          # Utility scripts (like build/deployment scripts)
â”œâ”€â”€ ğŸ¯ Makefile          # Automation commands (like npm scripts)
â”œâ”€â”€ ğŸ¤– benchmark_runner.py       # Main application entry point
â”œâ”€â”€ ğŸ“‹ README.md         # Project overview (standard in any repo)
â””â”€â”€ ğŸ§¹ Cleanup utilities  # Maintenance scripts
```

## ğŸ“Š **Data Directory** (`data/`)

**Purpose**: Reference datasets and cultural knowledge base
**Like**: Database fixtures, lookup tables, static assets in web apps

```
data/
â”œâ”€â”€ community/           # Community-contributed datasets
â”‚   â””â”€â”€ exports/        # Exported community data files
â””â”€â”€ cultural/           # Cultural reference data
    â”œâ”€â”€ traditions.json    # Cultural tradition definitions  
    â”œâ”€â”€ languages.json     # Language metadata
    â””â”€â”€ regions.json       # Geographic and cultural regions
```

### **Why This Structure?**
- **Read-only data**: These files are reference material, not modified by tests
- **Versioning**: Cultural data can be versioned and updated independently
- **Separation**: Community vs. built-in data clearly separated
- **Format**: JSON files for easy parsing and human readability

**Software Engineering Analogy**: Like having a `fixtures/` or `seed-data/` directory in a web application.

## ğŸ¯ **Domains Directory** (`domains/`)

**Purpose**: Test definitions organized by thinking type
**Like**: API route definitions, test specifications, configuration schemas

```
domains/
â”œâ”€â”€ reasoning/           # Logic and analysis tests
â”‚   â”œâ”€â”€ base_models/    # Tests for base/foundation models
â”‚   â””â”€â”€ instruct_models/ # Tests for instruction-tuned models
â”œâ”€â”€ creativity/          # Creative and artistic tasks
â”‚   â””â”€â”€ base_models/    # Creative tests for base models
â”œâ”€â”€ language/            # Linguistic and grammar tests
â”‚   â””â”€â”€ base_models/    # Language tests for base models
â”œâ”€â”€ social/              # Cultural and interpersonal tests
â”‚   â””â”€â”€ base_models/    # Social understanding tests
â”œâ”€â”€ knowledge/           # Factual knowledge tests
â”‚   â””â”€â”€ base_models/    # Knowledge tests for base models
â””â”€â”€ integration/         # Multi-domain complex tests
    â””â”€â”€ base_models/    # Integrated capability tests
```

### **Test Definition Structure**
Each domain contains JSON files like:
```json
{
  "test_id": "reasoning_logic_01",
  "category": "reasoning_general", 
  "difficulty": "medium",
  "prompt": "Analyze the logical consistency...",
  "metadata": {
    "estimated_time": 30,
    "requires_cultural_context": false
  }
}
```

**Software Engineering Analogy**: Like having separate route files for different API modules (`/users`, `/products`, `/orders`) in Express.js or Django.

## ğŸ§  **Evaluator Directory** (`evaluator/`)

**Purpose**: Business logic for scoring and analysis
**Like**: Service layer, middleware, plugin architecture in web frameworks

```
evaluator/
â”œâ”€â”€ core/                # Base classes and shared utilities
â”‚   â”œâ”€â”€ base_evaluator.py       # Interface definition (like abstract base class)
â”‚   â”œâ”€â”€ evaluation_result.py    # Result data structures  
â”‚   â””â”€â”€ scoring_utils.py        # Shared scoring functions
â”œâ”€â”€ subjects/            # Domain-specific evaluators
â”‚   â”œâ”€â”€ reasoning_evaluator.py  # Logic and analysis scoring
â”‚   â”œâ”€â”€ creativity_evaluator.py # Creative task evaluation
â”‚   â”œâ”€â”€ language_evaluator.py   # Linguistic analysis
â”‚   â””â”€â”€ social_evaluator.py     # Cultural understanding
â”œâ”€â”€ advanced/            # Sophisticated analysis tools
â”‚   â”œâ”€â”€ ensemble_evaluator.py   # Multiple evaluator coordination
â”‚   â”œâ”€â”€ disagreement_detector.py # Quality assurance checks
â”‚   â””â”€â”€ pattern_analyzer.py     # Statistical analysis tools
â”œâ”€â”€ cultural/            # Cultural context integration
â”‚   â”œâ”€â”€ cultural_validator.py   # Cultural appropriateness checks
â”‚   â”œâ”€â”€ tradition_analyzer.py   # Cultural tradition understanding
â”‚   â””â”€â”€ bias_detector.py        # Cultural bias detection
â”œâ”€â”€ validation/          # Quality assurance systems  
â”‚   â”œâ”€â”€ cross_validator.py      # Inter-evaluator consistency checks
â”‚   â”œâ”€â”€ edge_case_detector.py   # Unusual response handling
â”‚   â””â”€â”€ confidence_calculator.py # Scoring confidence estimation
â”œâ”€â”€ linguistics/         # Language-specific analysis
â”‚   â”œâ”€â”€ grammar_analyzer.py     # Grammar and syntax checking
â”‚   â”œâ”€â”€ semantic_analyzer.py    # Meaning and context analysis
â”‚   â””â”€â”€ multilingual_support.py # Multi-language capabilities
â””â”€â”€ data/               # Evaluator-specific data files
    â”œâ”€â”€ scoring_rubrics.json    # Standardized scoring criteria
    â”œâ”€â”€ cultural_patterns.json  # Cultural evaluation patterns
    â””â”€â”€ linguistic_rules.json   # Language analysis rules
```

### **Plugin Architecture Benefits**
- **Modularity**: Each evaluator handles one concern
- **Extensibility**: Easy to add new evaluation types
- **Testability**: Each component can be unit tested independently
- **Reusability**: Evaluators can be mixed and matched

**Software Engineering Analogy**: Like having separate service classes in a layered architecture, or middleware components in Express.js.

## âœ… **Tests Directory** (`tests/`)

**Purpose**: Framework quality assurance and validation
**Like**: Standard test directory structure in any software project

```
tests/
â”œâ”€â”€ unit/                # Component-level tests
â”‚   â”œâ”€â”€ test_evaluator.py          # Individual evaluator tests
â”‚   â”œâ”€â”€ test_scoring_utils.py      # Utility function tests
â”‚   â”œâ”€â”€ test_result_processing.py  # Result handling tests
â”‚   â””â”€â”€ test_config_loading.py     # Configuration tests
â”œâ”€â”€ integration/         # Multi-component tests
â”‚   â”œâ”€â”€ test_full_evaluation.py    # End-to-end evaluation tests
â”‚   â”œâ”€â”€ test_concurrent_execution.py # Parallel processing tests
â”‚   â””â”€â”€ test_api_integration.py    # External API integration tests
â”œâ”€â”€ functional/          # User workflow tests
â”‚   â”œâ”€â”€ test_cli_interface.py      # Command-line interface tests
â”‚   â”œâ”€â”€ test_result_reporting.py   # Output format tests
â”‚   â””â”€â”€ test_error_handling.py     # Error scenario tests
â”œâ”€â”€ analysis/            # Performance and quality analysis
â”‚   â”œâ”€â”€ analyze_scoring_patterns.py # Score distribution analysis
â”‚   â”œâ”€â”€ benchmark_performance.py   # Speed and resource usage
â”‚   â””â”€â”€ validate_consistency.py    # Cross-evaluator consistency
â””â”€â”€ results/             # Test output storage
    â”œâ”€â”€ unit_test_results/         # Unit test outputs
    â”œâ”€â”€ integration_results/       # Integration test outputs
    â””â”€â”€ performance_metrics/       # Performance test data
```

### **Test Strategy**
- **Unit Tests**: Test individual functions and classes
- **Integration Tests**: Test component interactions
- **Functional Tests**: Test user-facing features
- **Analysis Tests**: Validate system behavior and performance

**Software Engineering Analogy**: Standard test pyramid structure used in any mature software project.

## ğŸ“š **Documentation Directory** (`docs/`)

**Purpose**: Comprehensive project documentation
**Like**: Documentation site structure (Gitbook, Docusaurus, etc.)

```
docs/
â”œâ”€â”€ ARCHITECTURE.md                 # System design and patterns
â”œâ”€â”€ CONCEPTS.md                    # Core concepts explained
â”œâ”€â”€ DIRECTORY_STRUCTURE.md         # This file
â”œâ”€â”€ benchmark_test_runner_overview.md # Runner implementation details
â”œâ”€â”€ evaluation_system_overview.md  # Evaluation system deep dive
â”œâ”€â”€ integration_test_framework_plan.md # Test framework design
â”œâ”€â”€ TESTING_GUIDE.md               # How to run and write tests
â”œâ”€â”€ test_runner_interface.md       # API reference documentation
â”œâ”€â”€ test_schema_design.md          # Test definition schema
â””â”€â”€ validation_system_overview.py  # Validation system analysis
```

### **Documentation Strategy**
- **Progressive disclosure**: Start with concepts, then architecture, then details
- **Audience-specific**: Different docs for users vs. developers vs. operators
- **Living documentation**: Updated alongside code changes
- **Practical focus**: Examples and usage patterns, not just theory

## ğŸ”§ **Scripts Directory** (`scripts/`)

**Purpose**: Utility and automation scripts
**Like**: `scripts/` directory in Node.js projects, or `bin/` in Unix projects

```
scripts/
â”œâ”€â”€ setup_environment.py      # Development environment setup
â”œâ”€â”€ run_benchmarks.py         # Automated benchmark execution  
â”œâ”€â”€ export_results.py         # Result data export utilities
â”œâ”€â”€ validate_test_data.py     # Test data validation
â””â”€â”€ performance_profiling.py  # Performance analysis tools
```

**Software Engineering Analogy**: Like npm scripts, Makefile targets, or DevOps automation scripts.

## ğŸ“‹ **Root-Level Files**

### **Core Application Files**
- **`benchmark_runner.py`**: Main application entry point (like `app.py` in Flask, `server.js` in Node.js)
- **`run_tests_with_cleanup.py`**: Test execution wrapper with cleanup
- **`cleanup_test_artifacts.py`**: Maintenance utility for cleaning test artifacts

### **Project Management Files** 
- **`README.md`**: Project overview and quick start guide
- **`Makefile`**: Automation commands and workflows
- **`CLEANUP_GUIDE.md`**: Guide for managing test artifacts

### **Configuration Files** (when present)
- **`config.json`**: Application configuration
- **`requirements.txt`**: Python dependencies  
- **`.gitignore`**: Version control exclusions

## ğŸ”„ **Data Flow Through Directories**

### **Test Execution Flow**
```
1. domains/          â†’ Load test definitions
2. benchmark_runner.py â†’ Orchestrate execution
3. evaluator/        â†’ Process and score responses  
4. data/            â†’ Reference cultural/linguistic data
5. test_results/     â†’ Save scored results
```

### **Development Workflow**
```
1. tests/           â†’ Validate system functionality
2. docs/            â†’ Document changes and usage
3. scripts/         â†’ Automate common tasks
4. evaluator/       â†’ Implement new evaluation logic
5. domains/         â†’ Add new test cases
```

## ğŸ› ï¸ **Extension Points**

### **Adding New Domain Types**
1. Create new directory under `domains/new_domain/`
2. Add JSON test definitions following existing schema
3. No code changes needed - system auto-discovers

### **Adding New Evaluators**
1. Create new evaluator in appropriate `evaluator/` subdirectory
2. Implement `BaseEvaluator` interface
3. Add configuration mapping
4. Add unit tests in `tests/unit/`

### **Adding New Analysis Tools**
1. Create analysis script in `evaluator/advanced/`
2. Add integration tests in `tests/integration/`
3. Document usage in `docs/`
4. Add automation command to `Makefile`

## ğŸ” **Security and Isolation**

### **Read-Only Data**
- `data/` directory contains reference data that shouldn't be modified by tests
- Cultural and linguistic data versioned separately from code

### **Test Isolation**
- Each test execution creates isolated temporary artifacts
- Cleanup utilities ensure no test pollution between runs
- Results stored in timestamped directories

### **Configuration Security** 
- API endpoints and credentials configured via environment variables
- No hardcoded secrets in any configuration files
- Sensitive data excluded from version control

## ğŸ“ˆ **Scalability Considerations**

### **Horizontal Scaling**
- Test definitions can be distributed across multiple `domains/` 
- Evaluators can run independently and concurrently
- Results can be aggregated from distributed execution

### **Vertical Scaling**
- Large test suites can be partitioned by domain or difficulty
- Memory-intensive evaluations isolated in separate processes
- Cultural data can be cached and shared across evaluations

---

This directory structure follows **software engineering best practices**:
- **Single Responsibility**: Each directory has one clear purpose
- **Separation of Concerns**: Data, logic, tests, and docs are separate
- **Plugin Architecture**: Easy to extend without modifying existing code  
- **Configuration-Driven**: Behavior controlled by data, not hardcoded logic
- **Maintainable**: Clear organization makes the system easy to understand and modify

The structure scales from simple single evaluations to complex distributed benchmarking systems while maintaining clarity and maintainability.