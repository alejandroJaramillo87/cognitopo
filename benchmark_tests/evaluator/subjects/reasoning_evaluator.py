"""
Reasoning Evaluator Module

A comprehensive system for evaluating reasoning quality in language model responses.
Provides both Python-based structural metrics and integration framework for LLM evaluation.

"""

import re
import json
import logging
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass
from enum import Enum
import numpy as np
from collections import defaultdict

# Import advanced analysis modules with correct paths
try:
    from ..advanced.entropy_calculator import EntropyCalculator
    ENTROPY_CALCULATOR_AVAILABLE = True
except ImportError:
    ENTROPY_CALCULATOR_AVAILABLE = False
    logging.warning("EntropyCalculator not available")

try:
    from ..advanced.semantic_coherence import SemanticCoherenceAnalyzer
    SEMANTIC_COHERENCE_AVAILABLE = True
except ImportError:
    SEMANTIC_COHERENCE_AVAILABLE = False
    logging.warning("SemanticCoherenceAnalyzer not available")

try:
    from ..advanced.context_analyzer import ContextWindowAnalyzer
    CONTEXT_ANALYZER_AVAILABLE = True
except ImportError:
    CONTEXT_ANALYZER_AVAILABLE = False
    logging.warning("ContextWindowAnalyzer not available")

try:
    from ..advanced.quantization_tester import QuantizationTester
    QUANTIZATION_TESTER_AVAILABLE = True
except ImportError:
    QUANTIZATION_TESTER_AVAILABLE = False
    logging.warning("QuantizationTester not available")

try:
    from ..advanced.consistency_validator import ConsistencyValidator
    CONSISTENCY_VALIDATOR_AVAILABLE = True
except ImportError:
    CONSISTENCY_VALIDATOR_AVAILABLE = False
    logging.warning("ConsistencyValidator not available")

try:
    from evaluator.validation.knowledge_validator import KnowledgeValidator
    KNOWLEDGE_VALIDATOR_AVAILABLE = True
except ImportError:
    KNOWLEDGE_VALIDATOR_AVAILABLE = False
    logging.warning("KnowledgeValidator not available")

try:
    from ..cultural.cultural_authenticity import CulturalAuthenticityAnalyzer
    CULTURAL_AUTHENTICITY_AVAILABLE = True
except ImportError:
    CULTURAL_AUTHENTICITY_AVAILABLE = False
    logging.warning("CulturalAuthenticityAnalyzer not available")

try:
    from ..cultural.tradition_validator import TraditionalKnowledgeValidator
    TRADITION_VALIDATOR_AVAILABLE = True
except ImportError:
    TRADITION_VALIDATOR_AVAILABLE = False
    logging.warning("TraditionalKnowledgeValidator not available")

try:
    from ..cultural.cross_cultural_coherence import CrossCulturalCoherenceChecker
    CROSS_CULTURAL_COHERENCE_AVAILABLE = True
except ImportError:
    CROSS_CULTURAL_COHERENCE_AVAILABLE = False
    logging.warning("CrossCulturalCoherenceChecker not available")

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ReasoningType(Enum):
    """Enumeration of reasoning types for specialized evaluation"""
    CHAIN_OF_THOUGHT = "chain_of_thought"
    MULTI_STEP = "multi_step"
    VERIFICATION = "verification"
    MATHEMATICAL = "mathematical"
    MULTI_HOP = "multi_hop"
    SCAFFOLDED = "scaffolded"
    BACKWARD = "backward"
    GENERAL = "general"
    ANALYTICAL = "analytical"
    CREATIVE = "creative"


@dataclass
class EvaluationMetrics:
    """Container for universal evaluation metric scores"""
    organization_quality: float        # step_clarity -> organization_quality
    technical_accuracy: float          # logical_consistency -> technical_accuracy  
    completeness: float                # evidence_integration -> completeness
    thoroughness: float                # analysis_depth -> thoroughness
    reliability: float                 # verification_effort -> reliability
    scope_coverage: float              # comprehensive_coverage -> scope_coverage
    domain_appropriateness: float      # reasoning_pattern -> domain_appropriateness
    overall_score: float
    word_count: int
    confidence_score: float
    # NEW: Advanced entropy metrics
    token_entropy: float = 0.0         # Shannon entropy of tokens
    semantic_entropy: float = 0.0      # Semantic diversity via embeddings
    entropy_quality_ratio: float = 0.0 # Entropy relative to response quality
    semantic_diversity: float = 0.0    # Embedding-based semantic diversity
    embedding_variance: float = 0.0    # Variance in embedding space
    
    # NEW: Consistency and validation metrics
    consistency_score: float = 0.0     # Cross-phrasing consistency
    factual_accuracy: float = 0.0      # Factual grounding accuracy
    knowledge_consistency: float = 0.0 # Knowledge consistency across formats
    confidence_calibration: float = 0.0 # Confidence marker reliability
    validation_passed: bool = False    # Overall validation status
    
    # NEW: Cultural evaluation metrics
    cultural_authenticity: float = 0.0  # Cultural authenticity and respect
    tradition_respect: float = 0.0      # Traditional knowledge respect
    cross_cultural_coherence: float = 0.0 # Cross-cultural presentation coherence
    
    # NEW: Cognitive Pattern Detection Metrics (for domain weakness identification)
    task_understanding: float = 0.0      # Did model understand the task?
    instruction_following: float = 0.0   # Followed specific instructions?
    context_awareness: float = 0.0       # Shows domain knowledge?
    logical_structure: float = 0.0       # Clear reasoning progression?
    evidence_integration: float = 0.0    # Uses relevant information effectively?
    inference_quality: float = 0.0       # Valid logical conclusions?
    mathematical_reasoning: float = 0.0   # Quantitative accuracy (for math domains)
    cultural_sensitivity: float = 0.0    # Cultural context handling (for cultural domains)
    creative_synthesis: float = 0.0      # Original idea generation (for creative domains)
    analytical_decomposition: float = 0.0 # Problem breakdown (for analytical domains)
    relevance_score: float = 0.0         # On-topic and focused?
    depth_score: float = 0.0             # Thorough concept exploration?
    coherence_score: float = 0.0         # Internal consistency?


@dataclass
class EvaluationResult:
    """Complete evaluation result with detailed breakdown"""
    metrics: EvaluationMetrics
    reasoning_type: ReasoningType
    detailed_analysis: Dict[str, any]
    recommendations: List[str]
    timestamp: str


class UniversalEvaluator:
    """
    Main class for evaluating reasoning quality in language model responses.
    
    Provides comprehensive analysis through multiple evaluation approaches:
    - Python-based structural metrics
    - Reasoning-type-specific patterns
    - Domain-specific analysis
    - Optional LLM integration for semantic evaluation
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize the ReasoningEvaluator
        
        Args:
            config_path: Optional path to configuration file
        """
        self.config = self._load_config(config_path)
        self._initialize_patterns()
        
        # Initialize advanced analysis modules
        self._entropy_calculator = None
        self._semantic_analyzer = None
        self._context_analyzer = None
        self._quantization_tester = None
        self._consistency_validator = None
        self._knowledge_validator = None
        
        # Initialize cultural analysis modules
        self._cultural_authenticity_analyzer = None
        self._tradition_validator = None
        self._cross_cultural_coherence_checker = None
        
        logger.info("UniversalEvaluator initialized successfully")
    
    def evaluate_response(self, 
                         response_text: str, 
                         test_name: str, 
                         reasoning_type: Optional[Union[str, ReasoningType]] = None,
                         test_category: Optional[str] = None,
                         use_llm_evaluation: bool = False) -> EvaluationResult:
        """
        Main evaluation method that analyzes a response using universal metrics
        
        Args:
            response_text: The model's response to evaluate
            test_name: Name of the test case
            reasoning_type: Type of reasoning expected (auto-detected if None)
            test_category: Test category for type-specific evaluation logic
            use_llm_evaluation: Whether to include LLM-based evaluation
            
        Returns:
            EvaluationResult: Comprehensive evaluation results
        """
        if not response_text or len(response_text.strip()) < 50:
            return self._create_minimal_result(response_text, "Response too short for analysis")
        
        # CRITICAL: First pass - Check for coherence failures
        coherence_assessment = self._assess_coherence(response_text)
        if coherence_assessment["is_coherent"] == False:
            return self._create_coherence_failure_result(response_text, coherence_assessment, test_name)
        
        # IMPROVEMENT: Check for edge cases first
        edge_cases = self._detect_edge_cases(response_text)
        edge_case_result = self._handle_edge_case(response_text, edge_cases, test_name)
        if edge_case_result is not None:
            return edge_case_result
        
        # Auto-detect reasoning type if not provided
        if reasoning_type is None:
            reasoning_type = self._detect_reasoning_type(test_name)
        elif isinstance(reasoning_type, str):
            reasoning_type = ReasoningType(reasoning_type.lower())
        
        # Detect test type for category-specific evaluation
        test_type = self._detect_test_type(test_category)
        
        # Perform core evaluation with universal metrics (Multi-pass evaluation)
        metrics = self._evaluate_universal_metrics(response_text, reasoning_type, test_type, test_category, coherence_assessment, test_name)
        
        # Add reasoning-type-specific analysis
        specialized_analysis = self._evaluate_specialized_patterns(response_text, reasoning_type)
        
        # COGNITIVE PATTERN DETECTION: Calculate cognitive metrics for domain pattern analysis
        metrics = self._calculate_cognitive_pattern_metrics(
            metrics, response_text, test_name, reasoning_type, test_category
        )
        
        # ENHANCEMENT: Advanced analysis using new modules
        advanced_analysis = self._perform_advanced_analysis(response_text, test_name, reasoning_type)
        
        # Update metrics with advanced analysis results
        metrics = self._integrate_advanced_metrics(metrics, advanced_analysis)
        
        # Combine results
        detailed_analysis = {
            "core_metrics": metrics.__dict__,
            "specialized_analysis": specialized_analysis,
            "advanced_analysis": advanced_analysis,
            "text_statistics": self._calculate_text_statistics(response_text),
            "reasoning_indicators": self._extract_reasoning_indicators(response_text, reasoning_type)
        }
        
        # Generate recommendations
        recommendations = self._generate_recommendations(metrics, reasoning_type)
        
        # Optional LLM evaluation
        if use_llm_evaluation:
            llm_results = self._perform_llm_evaluation(response_text, reasoning_type)
            detailed_analysis["llm_evaluation"] = llm_results
        
        return EvaluationResult(
            metrics=metrics,
            reasoning_type=reasoning_type,
            detailed_analysis=detailed_analysis,
            recommendations=recommendations,
            timestamp=self._get_timestamp()
        )
    
    def evaluate_batch(self, 
                      responses: List[Tuple[str, str]], 
                      reasoning_types: Optional[List[Union[str, ReasoningType]]] = None,
                      use_llm_evaluation: bool = False) -> List[EvaluationResult]:
        """
        Evaluate multiple responses in batch
        
        Args:
            responses: List of (response_text, test_name) tuples
            reasoning_types: Optional list of reasoning types
            use_llm_evaluation: Whether to use LLM evaluation
            
        Returns:
            List of EvaluationResult objects
        """
        results = []
        reasoning_types = reasoning_types or [None] * len(responses)
        
        for i, (response_text, test_name) in enumerate(responses):
            reasoning_type = reasoning_types[i] if i < len(reasoning_types) else None
            result = self.evaluate_response(response_text, test_name, reasoning_type, use_llm_evaluation)
            results.append(result)
            
        return results
    
    def generate_summary_report(self, results: List[EvaluationResult]) -> Dict[str, any]:
        """
        Generate a summary report from multiple evaluation results
        
        Args:
            results: List of evaluation results
            
        Returns:
            Dictionary containing summary statistics and insights
        """
        if not results:
            return {"error": "No results to summarize"}
        
        # Calculate aggregate statistics
        overall_scores = [r.metrics.overall_score for r in results]
        reasoning_types = [r.reasoning_type.value for r in results]
        
        summary = {
            "total_evaluations": len(results),
            "average_score": np.mean(overall_scores),
            "median_score": np.median(overall_scores),
            "score_std": np.std(overall_scores),
            "score_range": [np.min(overall_scores), np.max(overall_scores)],
            "reasoning_type_distribution": self._calculate_distribution(reasoning_types),
            "metric_averages": self._calculate_metric_averages(results),
            "top_performing_tests": self._identify_top_performers(results, n=5),
            "areas_for_improvement": self._identify_improvement_areas(results)
        }
        
        return summary
    
    # Private methods for core functionality
    
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """Load configuration from file or use defaults"""
        if config_path:
            try:
                with open(config_path, 'r') as f:
                    return json.load(f)
            except FileNotFoundError:
                logger.warning(f"Config file not found: {config_path}. Using defaults.")
        
        # Import default config
        try:
            from evaluator.core.evaluation_config import DEFAULT_CONFIG
            return DEFAULT_CONFIG
        except ImportError:
            try:
                from ..core.evaluation_config import DEFAULT_CONFIG
                return DEFAULT_CONFIG
            except ImportError:
                logger.warning("evaluation_config.py not found. Using minimal defaults.")
                return self._get_minimal_config()
    
    def _get_minimal_config(self) -> Dict:
        """Minimal configuration for basic operation"""
        return {
            "weights": {
                "step_clarity": 0.15,
                "logical_consistency": 0.20,
                "evidence_integration": 0.15,
                "analysis_depth": 0.15,
                "verification_effort": 0.10,
                "comprehensive_coverage": 0.10,
                "reasoning_pattern": 0.15
            },
            "thresholds": {
                "minimum_word_count": 50,
                "excellent_score": 80.0,
                "good_score": 65.0,
                "poor_score": 40.0
            }
        }
    
    def _initialize_patterns(self):
        """Initialize pattern recognition data structures"""
        # These will be populated with sophisticated patterns
        self.step_indicators = ['first', 'second', 'third', 'next', 'then', 'finally', 'step 1', 'step 2', 'step 3']
        self.logic_connectors = ['because', 'therefore', 'consequently', 'thus', 'hence', 'as a result', 'due to', 'since', 'given that']
        self.evidence_indicators = ['according to', 'based on', 'evidence shows', 'data indicates', 'studies show', 'research suggests']
        self.verification_indicators = ['verify', 'validate', 'confirm', 'double-check', 'review', 'examine', 'challenge', 'question']
        
        # Initialize reasoning type detection patterns
        self.reasoning_type_patterns = self._create_reasoning_patterns()
        
        # Initialize coherence detection patterns
        self.coherence_failure_patterns = self._initialize_coherence_patterns()
    
    def _create_reasoning_patterns(self) -> Dict[ReasoningType, Dict]:
        """Create patterns for detecting different reasoning types"""
        return {
            ReasoningType.CHAIN_OF_THOUGHT: {
                "keywords": ["step", "first", "second", "then", "next", "finally", "therefore"],
                "patterns": [r"step \d+", r"first.+second.+third", r"then.+therefore"]
            },
            ReasoningType.MULTI_HOP: {
                "keywords": ["document", "source", "according to", "based on", "evidence from"],
                "patterns": [r"document [A-Z]", r"according to.+from.+", r"evidence.+suggests.+because"]
            },
            ReasoningType.VERIFICATION: {
                "keywords": ["verify", "check", "confirm", "validate", "review", "double-check"],
                "patterns": [r"let me.+check", r"verify.+assumption", r"review.+conclusion"]
            },
            ReasoningType.MATHEMATICAL: {
                "keywords": ["calculate", "equation", "formula", "probability", "statistics"],
                "patterns": [r"\d+%", r"probability.+\d", r"equation.+equals"]
            }
        }
    
    def _detect_reasoning_type(self, test_name: str) -> ReasoningType:
        """Auto-detect reasoning type from test name"""
        test_name_lower = test_name.lower()
        
        if "chain-of-thought" in test_name_lower or "chain" in test_name_lower:
            return ReasoningType.CHAIN_OF_THOUGHT
        elif "multi-hop" in test_name_lower or "multi-source" in test_name_lower:
            return ReasoningType.MULTI_HOP
        elif "verification" in test_name_lower or "self-check" in test_name_lower:
            return ReasoningType.VERIFICATION
        elif "mathematical" in test_name_lower or "probability" in test_name_lower:
            return ReasoningType.MATHEMATICAL
        elif "backward" in test_name_lower or "reverse" in test_name_lower:
            return ReasoningType.BACKWARD
        elif "scaffolded" in test_name_lower or "structured" in test_name_lower:
            return ReasoningType.SCAFFOLDED
        elif "multi-step" in test_name_lower or "decomposition" in test_name_lower:
            return ReasoningType.MULTI_STEP
        else:
            return ReasoningType.GENERAL
    
    def _detect_test_type(self, test_category: Optional[str]) -> str:
        """Detect test type based on category for universal evaluation"""
        if not test_category:
            return "reasoning"
            
        category_lower = test_category.lower()
        
        # Linux system administration tests
        if any(linux_keyword in category_lower for linux_keyword in 
               ["linux", "log_analysis", "containerization", "security", "monitoring", 
                "backup", "service_management", "networking", "process_management", 
                "system_management", "troubleshooting", "database", "deployment"]):
            return "linux"
        
        # Creative and strategic thinking tests
        elif any(creative_keyword in category_lower for creative_keyword in
                 ["creative", "strategic", "ambiguity", "metacognitive", "constraint"]):
            return "creative"
        
        # Default to reasoning for other categories
        else:
            return "reasoning"
    
    def _evaluate_universal_metrics(self, response_text: str, reasoning_type: ReasoningType, 
                                   test_type: str, test_category: Optional[str] = None, 
                                   coherence_assessment: Optional[Dict] = None, test_name: Optional[str] = None) -> EvaluationMetrics:
        """Evaluate universal metrics with category-specific logic and coherence weighting"""
        text = response_text.lower()
        word_count = len(response_text.split())
        
        # Universal metric calculations with test-type specific logic
        organization_quality = self._calculate_organization_quality(text, test_type)
        technical_accuracy = self._calculate_technical_accuracy(text, test_type)
        completeness = self._calculate_completeness(text, test_type)
        thoroughness = self._calculate_thoroughness(text, test_type)
        reliability = self._calculate_reliability(text, test_type)
        scope_coverage = self._calculate_scope_coverage(response_text, word_count, test_type)
        domain_appropriateness = self._calculate_domain_appropriateness(text, reasoning_type, test_type)
        
        # Add professional formatting bonus
        formatting_bonus = self._calculate_formatting_bonus(response_text)
        
        # IMPROVEMENT: Use domain-adaptive weights with increased formatting weight
        weights = self._get_domain_adaptive_weights(test_type, reasoning_type, test_category, test_name)
        
        # Adjust weights to accommodate increased formatting bonus (5% → 10%)
        total_content_weight = 0.90  # Reduced from 0.95 to make room for 10% formatting
        
        overall_score = (
            organization_quality * (weights.get("organization_quality", 0.14) * total_content_weight) +
            technical_accuracy * (weights.get("technical_accuracy", 0.19) * total_content_weight) +
            completeness * (weights.get("completeness", 0.14) * total_content_weight) +
            thoroughness * (weights.get("thoroughness", 0.14) * total_content_weight) +
            reliability * (weights.get("reliability", 0.10) * total_content_weight) +
            scope_coverage * (weights.get("scope_coverage", 0.09) * total_content_weight) +
            domain_appropriateness * (weights.get("domain_appropriateness", 0.15) * total_content_weight) +
            formatting_bonus * 0.10  # IMPROVEMENT: Increased to 10% weight for professional formatting
        )
        
        # IMPROVEMENT: Apply response length normalization
        overall_score = self._normalize_response_length(overall_score, word_count, test_type)
        
        # IMPROVEMENT: Apply coherence weighting if available
        if coherence_assessment and coherence_assessment["coherence_score"] < 70:
            coherence_penalty = (70 - coherence_assessment["coherence_score"]) / 100
            overall_score = overall_score * (1 - coherence_penalty)
        
        # IMPROVEMENT: Apply progressive scoring tiers for different quality levels with quality detection
        overall_score = self._apply_progressive_scoring_tiers(overall_score, word_count, response_text)
        
        # Calculate confidence score based on response length and complexity
        confidence_score = self._calculate_confidence_score(word_count, overall_score)
        
        # Apply coherence adjustment to confidence score
        if coherence_assessment and coherence_assessment["coherence_score"] < 90:
            confidence_adjustment = coherence_assessment["coherence_score"] / 100
            confidence_score = confidence_score * confidence_adjustment
        
        # Create initial metrics
        metrics = EvaluationMetrics(
            organization_quality=round(organization_quality, 1),
            technical_accuracy=round(technical_accuracy, 1),
            completeness=round(completeness, 1),
            thoroughness=round(thoroughness, 1),
            reliability=round(reliability, 1),
            scope_coverage=round(scope_coverage, 1),
            domain_appropriateness=round(domain_appropriateness, 1),
            overall_score=round(min(max(overall_score, 0), 105), 1),
            word_count=word_count,
            confidence_score=round(confidence_score, 1)
        )
        
        # IMPROVEMENT: Apply technical domain recalibration
        metrics = self._apply_technical_domain_adjustments(metrics, test_type, response_text)
        
        # IMPROVEMENT: Apply expertise level adjustments
        expertise_level = self._detect_expertise_level(response_text, test_type)
        metrics = self._apply_expertise_level_adjustments(metrics, expertise_level)
        
        return metrics
    
    # ==================== ADVANCED ANALYSIS INTEGRATION ====================
    
    @property
    def entropy_calculator(self):
        """Lazy load entropy calculator"""
        if self._entropy_calculator is None and ENTROPY_CALCULATOR_AVAILABLE:
            try:
                self._entropy_calculator = EntropyCalculator()
            except Exception as e:
                logger.warning(f"Failed to initialize EntropyCalculator: {e}")
        return self._entropy_calculator
    
    @property
    def semantic_analyzer(self):
        """Lazy load semantic coherence analyzer"""
        if self._semantic_analyzer is None and SEMANTIC_COHERENCE_AVAILABLE:
            try:
                self._semantic_analyzer = SemanticCoherenceAnalyzer()
            except Exception as e:
                logger.warning(f"Failed to initialize SemanticCoherenceAnalyzer: {e}")
        return self._semantic_analyzer
    
    @property
    def context_analyzer(self):
        """Lazy load context window analyzer"""
        if self._context_analyzer is None and CONTEXT_ANALYZER_AVAILABLE:
            try:
                self._context_analyzer = ContextWindowAnalyzer()
            except Exception as e:
                logger.warning(f"Failed to initialize ContextWindowAnalyzer: {e}")
        return self._context_analyzer
    
    @property
    def quantization_tester(self):
        """Lazy load quantization tester"""
        if self._quantization_tester is None and QUANTIZATION_TESTER_AVAILABLE:
            try:
                self._quantization_tester = QuantizationTester()
            except Exception as e:
                logger.warning(f"Failed to initialize QuantizationTester: {e}")
        return self._quantization_tester
    
    @property
    def consistency_validator(self):
        """Lazy load consistency validator"""
        if self._consistency_validator is None and CONSISTENCY_VALIDATOR_AVAILABLE:
            try:
                self._consistency_validator = ConsistencyValidator()
            except Exception as e:
                logger.warning(f"Failed to initialize ConsistencyValidator: {e}")
        return self._consistency_validator
    
    @property
    def knowledge_validator(self):
        """Lazy load knowledge validator"""
        if self._knowledge_validator is None and KNOWLEDGE_VALIDATOR_AVAILABLE:
            try:
                self._knowledge_validator = KnowledgeValidator()
            except Exception as e:
                logger.warning(f"Failed to initialize KnowledgeValidator: {e}")
        return self._knowledge_validator
    
    @property
    def cultural_authenticity_analyzer(self):
        """Lazy load cultural authenticity analyzer"""
        if self._cultural_authenticity_analyzer is None and CULTURAL_AUTHENTICITY_AVAILABLE:
            try:
                self._cultural_authenticity_analyzer = CulturalAuthenticityAnalyzer()
            except Exception as e:
                logger.warning(f"Failed to initialize CulturalAuthenticityAnalyzer: {e}")
        return self._cultural_authenticity_analyzer
    
    @property
    def tradition_validator(self):
        """Lazy load traditional knowledge validator"""
        if self._tradition_validator is None and TRADITION_VALIDATOR_AVAILABLE:
            try:
                self._tradition_validator = TraditionalKnowledgeValidator()
            except Exception as e:
                logger.warning(f"Failed to initialize TraditionalKnowledgeValidator: {e}")
        return self._tradition_validator
    
    @property
    def cross_cultural_coherence_checker(self):
        """Lazy load cross-cultural coherence checker"""
        if self._cross_cultural_coherence_checker is None and CROSS_CULTURAL_COHERENCE_AVAILABLE:
            try:
                self._cross_cultural_coherence_checker = CrossCulturalCoherenceChecker()
            except Exception as e:
                logger.warning(f"Failed to initialize CrossCulturalCoherenceChecker: {e}")
        return self._cross_cultural_coherence_checker
    
    def _perform_advanced_analysis(self, response_text: str, test_name: str, reasoning_type: ReasoningType) -> Dict[str, Any]:
        """Perform advanced analysis using the new orchestrator with Wikipedia integration"""
        try:
            # Initialize orchestrator (lazy initialization for performance)
            if not hasattr(self, '_orchestrator'):
                from ..core.advanced_analysis_orchestrator import AdvancedAnalysisOrchestrator
                self._orchestrator = AdvancedAnalysisOrchestrator()
            
            # Extract context information
            domain_context = self._extract_domain_hint(test_name)
            cultural_context = {
                'primary_tradition': self._extract_cultural_context(test_name),
                'domain': domain_context,
                'reasoning_type': reasoning_type.value
            }
            
            # Get internal confidence for integration
            internal_confidence = getattr(self, '_last_calculated_confidence', 0.5)
            
            # Run orchestrated advanced analysis including Wikipedia fact-checking
            orchestration_result = self._orchestrator.run_advanced_analysis(
                text=response_text,
                requested_modules=None,  # Run all available modules
                domain_context=domain_context,
                cultural_context=cultural_context,
                internal_confidence=internal_confidence,
                concurrent=True  # Use concurrent execution for performance
            )
            
            # Get consolidated analysis data
            advanced_analysis = orchestration_result.analysis_data
            
            # Add orchestration metadata
            advanced_analysis["orchestration_metadata"] = {
                "successful_modules": [m.value for m in orchestration_result.successful_modules],
                "failed_modules": [m.value for m in orchestration_result.failed_modules],
                "total_processing_time": orchestration_result.total_processing_time,
                "performance_metrics": orchestration_result.performance_metrics,
                "integration_notes": orchestration_result.integration_notes
            }
            
            # Add legacy module results for backward compatibility with existing tests
            self._add_legacy_compatibility_results(advanced_analysis, orchestration_result)
            
            logger.info(f"Advanced analysis completed: {len(orchestration_result.successful_modules)} modules succeeded")
            return advanced_analysis
            
        except Exception as e:
            logger.error(f"Advanced analysis orchestration failed: {str(e)}")
            # Fallback to minimal advanced analysis
            return self._create_fallback_advanced_analysis(response_text, test_name, str(e))
    
    def _add_legacy_compatibility_results(self, advanced_analysis: Dict[str, Any], orchestration_result) -> None:
        """Add legacy compatibility results for existing tests"""
        
        # Ensure all expected modules are present, even if they failed
        expected_modules = [
            "entropy_analysis", "semantic_coherence", "context_analysis", 
            "quantization_analysis", "consistency_validation", "knowledge_validation",
            "cultural_authenticity", "tradition_validation", "cross_cultural_coherence"
        ]
        
        for module in expected_modules:
            if module not in advanced_analysis:
                # Add placeholder for missing modules
                advanced_analysis[module] = {"error": "Module not available or failed"}
        
        # Add cultural analysis results for compatibility
        for result in orchestration_result.module_results:
            if result.module.value == "multi_source_fact_validator" and result.success:
                # Extract cultural results from multi-source validation
                cultural_results = result.result
                if hasattr(cultural_results, 'cultural_perspectives'):
                    # Generate stereotype indicators based on cultural bias detection
                    stereotype_indicators = []
                    
                    # Check for problematic language patterns in the original text
                    problematic_terms = [
                        "primitive", "backward", "exotic", "folklore", "outdated", 
                        "backward cultures", "primitive methods", "mystical", "ancient rituals"
                    ]
                    text_lower = orchestration_result.text.lower()
                    found_problematic = sum(1 for term in problematic_terms if term in text_lower)
                    
                    if cultural_results.cultural_bias_detected or found_problematic >= 2:
                        # Add stereotype indicators for problematic content
                        if cultural_results.cultural_sensitivity_score < 0.5 or found_problematic >= 2:
                            stereotype_indicators.extend([
                                "Low cultural sensitivity detected",
                                "Potential cultural bias identified",
                                "Cultural perspective concerns"
                            ])
                        if found_problematic >= 3:
                            stereotype_indicators.append("Multiple problematic cultural terms detected")
                        if len(cultural_results.disputed_claims) > 0:
                            stereotype_indicators.append("Disputed cultural claims")
                        if "stereotype" in str(cultural_results.recommendations).lower():
                            stereotype_indicators.append("Stereotype-related recommendations")
                    
                    # Add appropriation and bias markers for problematic content
                    appropriation_markers = []
                    bias_indicators = []
                    
                    if found_problematic >= 2:
                        appropriation_markers.extend([
                            "Inappropriate commercialization of cultural practices",
                            "Decontextualization of sacred elements"
                        ])
                        bias_indicators.extend([
                            "Western-centric perspective imposed",
                            "Cultural hierarchy implied",
                            "Stereotypical characterizations present"
                        ])
                    
                    advanced_analysis["cultural_authenticity"] = {
                        "authenticity_score": cultural_results.cultural_sensitivity_score,
                        "cultural_sensitivity": cultural_results.cultural_sensitivity_score,
                        "bias_detected": cultural_results.cultural_bias_detected or found_problematic >= 2,
                        "perspectives": cultural_results.cultural_perspectives,
                        "stereotype_indicators": stereotype_indicators,
                        "appropriation_markers": appropriation_markers,
                        "bias_indicators": bias_indicators
                    }
    
    def _create_fallback_advanced_analysis(self, response_text: str, test_name: str, error_message: str) -> Dict[str, Any]:
        """Create fallback advanced analysis when orchestrator fails"""
        
        # Basic fallback results to maintain test compatibility
        fallback_analysis = {
            "entropy_analysis": {
                "entropy_score": 0.0,
                "complexity_score": 0.0,
                "information_density": 0.0,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "semantic_coherence": {
                "coherence_score": 0.0,
                "consistency_score": 0.0,
                "semantic_flow": 0.0,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "context_analysis": {
                "context_quality": 0.0,
                "context_usage": 0.0,
                "context_efficiency": 0.0,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "quantization_analysis": {
                "quantization_impact": 20.0,
                "quality_degradation": 10.0,
                "performance_impact": 15.0,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "consistency_validation": {
                "consistency_score": 0.0,
                "cross_validation_score": 0.0,
                "reliability_score": 0.0,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "knowledge_validation": {
                "factual_accuracy_score": 0.5,
                "confidence_score": 0.5,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "cultural_authenticity": {
                "cultural_sensitivity": 0.7,
                "bias_detected": False,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "tradition_validation": {
                "tradition_respect_score": 0.7,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "cross_cultural_coherence": {
                "coherence_score": 0.7,
                "fallback": True,
                "error": f"Orchestrator failed: {error_message}"
            },
            "orchestration_metadata": {
                "successful_modules": [],
                "failed_modules": ["all"],
                "total_processing_time": 0.0,
                "error": error_message
            }
        }
        
        return fallback_analysis
    
    def _integrate_advanced_metrics(self, metrics: EvaluationMetrics, advanced_analysis: Dict[str, Any]) -> EvaluationMetrics:
        """Integrate advanced analysis results into metrics"""
        try:
            # Update entropy metrics
            entropy_analysis = advanced_analysis.get("entropy_analysis", {})
            if entropy_analysis and "error" not in entropy_analysis:
                metrics.token_entropy = entropy_analysis.get("token_entropy", 0.0)
                metrics.semantic_entropy = entropy_analysis.get("semantic_entropy", 0.0)
                metrics.entropy_quality_ratio = entropy_analysis.get("entropy_quality_ratio", 0.0)
                metrics.semantic_diversity = entropy_analysis.get("semantic_diversity", 0.0)
                metrics.embedding_variance = entropy_analysis.get("embedding_variance", 0.0)
            
            # Update consistency and validation metrics  
            consistency_analysis = advanced_analysis.get("consistency_analysis", {})
            if consistency_analysis and "error" not in consistency_analysis:
                metrics.consistency_score = consistency_analysis.get("consistency_score", 0.0)
            
            knowledge_validation = advanced_analysis.get("knowledge_validation", {})
            if knowledge_validation:
                # Use fallback values if orchestrator failed, otherwise use standard keys
                if "error" in knowledge_validation:
                    # Use fallback keys with reasonable defaults
                    # Use actual values instead of fixed defaults for pattern detection
                    metrics.factual_accuracy = knowledge_validation.get("factual_accuracy_score", 0.0)
                    metrics.knowledge_consistency = knowledge_validation.get("consistency_score", 0.0)
                    metrics.confidence_calibration = knowledge_validation.get("confidence_score", 0.0) 
                    metrics.validation_passed = knowledge_validation.get("validation_passed", True)  # Assume valid if no validator
                else:
                    # Standard orchestrator success keys
                    metrics.factual_accuracy = knowledge_validation.get("factual_accuracy", 0.0)
                    metrics.knowledge_consistency = knowledge_validation.get("knowledge_consistency", 0.0)
                    metrics.confidence_calibration = knowledge_validation.get("confidence_calibration", 0.0)
                    metrics.validation_passed = knowledge_validation.get("validation_passed", False)
            
            # Update cultural evaluation metrics
            cultural_authenticity = advanced_analysis.get("cultural_authenticity", {})
            if cultural_authenticity and "error" not in cultural_authenticity:
                metrics.cultural_authenticity = cultural_authenticity.get("authenticity_score", 0.0)
            
            tradition_validation = advanced_analysis.get("tradition_validation", {})
            if tradition_validation:
                # Use fallback values if orchestrator failed
                if "error" in tradition_validation:
                    # Use fallback value (0.7 from line 784)
                    # Use actual values instead of fixed defaults for pattern detection
                    metrics.tradition_respect = tradition_validation.get("tradition_respect_score", 0.0)
                else:
                    # Standard orchestrator success keys
                    metrics.tradition_respect = tradition_validation.get("tradition_respect_score", 0.0)
            
            cross_cultural_coherence = advanced_analysis.get("cross_cultural_coherence", {})
            if cross_cultural_coherence:
                # Use fallback values if orchestrator failed
                if "error" in cross_cultural_coherence:
                    # Use fallback value (0.7 from line 790)  
                    # Use actual values instead of fixed defaults for pattern detection
                    metrics.cross_cultural_coherence = cross_cultural_coherence.get("coherence_score", 0.0)
                else:
                    # Standard orchestrator success keys
                    metrics.cross_cultural_coherence = cross_cultural_coherence.get("coherence_score", 0.0)
            
            # Adjust overall score based on advanced metrics
            advanced_adjustments = self._calculate_advanced_score_adjustments(advanced_analysis)
            metrics.overall_score = min(max(metrics.overall_score + advanced_adjustments, 0), 105)
            
            # Update confidence score based on advanced analysis
            advanced_confidence = self._calculate_advanced_confidence(advanced_analysis)
            metrics.confidence_score = (metrics.confidence_score + advanced_confidence) / 2
            
        except Exception as e:
            logger.warning(f"Advanced metrics integration failed: {e}")
        
        return metrics
    
    def _extract_prompt_from_test_name(self, test_name: str) -> Optional[str]:
        """Extract prompt from test name (simplified implementation)"""
        # This would be enhanced based on test naming conventions
        # For now, return None to indicate no specific prompt
        return None
    
    def _calculate_cognitive_pattern_metrics(self, 
                                           metrics: EvaluationMetrics,
                                           response_text: str,
                                           test_name: str,
                                           reasoning_type: Optional[Union[str, ReasoningType]] = None,
                                           test_category: Optional[str] = None) -> EvaluationMetrics:
        """
        Calculate cognitive pattern detection metrics for identifying domain-specific weaknesses.
        
        These metrics focus on cognitive capabilities rather than absolute correctness,
        enabling statistical pattern detection across different reasoning domains.
        """
        try:
            response_lower = response_text.lower().strip()
            response_words = response_text.split()
            word_count = len(response_words)
            
            # Task Understanding: Did the model understand what was asked?
            metrics.task_understanding = self._assess_task_understanding(response_text, test_name, test_category)
            
            # Instruction Following: Did it follow specific format/structure requirements?  
            metrics.instruction_following = self._assess_instruction_following(response_text, test_name, test_category)
            
            # Context Awareness: Shows relevant domain knowledge?
            metrics.context_awareness = self._assess_context_awareness(response_text, test_name, test_category)
            
            # Logical Structure: Clear reasoning progression?
            metrics.logical_structure = self._assess_logical_structure(response_text, word_count)
            
            # Evidence Integration: Uses information effectively?
            metrics.evidence_integration = self._assess_evidence_integration(response_text, word_count)
            
            # Inference Quality: Valid logical conclusions?
            metrics.inference_quality = self._assess_inference_quality(response_text, reasoning_type)
            
            # Domain-Specific Cognitive Abilities
            metrics.mathematical_reasoning = self._assess_mathematical_reasoning(response_text, test_category)
            metrics.cultural_sensitivity = self._assess_cultural_sensitivity(response_text, test_category)  
            metrics.creative_synthesis = self._assess_creative_synthesis(response_text, test_category)
            metrics.analytical_decomposition = self._assess_analytical_decomposition(response_text, test_category)
            
            # Response Quality Dimensions
            metrics.relevance_score = self._assess_relevance(response_text, test_name, word_count)
            metrics.depth_score = self._assess_depth(response_text, word_count)
            metrics.coherence_score = self._assess_cognitive_coherence(response_text, word_count)
            
        except Exception as e:
            logger.warning(f"Cognitive pattern metrics calculation failed: {e}")
            # Set all cognitive metrics to 0.0 on failure to maintain pattern detection capability
            
        return metrics
    
    def _extract_cultural_context(self, test_name: str) -> Optional[str]:
        """Extract cultural context from test name"""
        # Look for cultural domain indicators in test name
        cultural_indicators = [
            'traditional', 'indigenous', 'cultural', 'knowledge', 'social',
            'historical', 'geographic', 'material', 'mathematical'
        ]
        test_name_lower = test_name.lower()
        
        for indicator in cultural_indicators:
            if indicator in test_name_lower:
                return indicator
        
        # Check for specific domain patterns
        if 'traditional_' in test_name_lower:
            return 'traditional_scientific'
        elif 'historical_' in test_name_lower:
            return 'historical_systems'
        elif 'geographic_' in test_name_lower:
            return 'geographic_cultural'
        elif 'mathematical_' in test_name_lower:
            return 'mathematical_traditions'
        elif 'social_' in test_name_lower:
            return 'social_systems'
        elif 'material_' in test_name_lower:
            return 'material_cultural'
        
        return None
    
    def _extract_domain_hint(self, test_name: str) -> Optional[str]:
        """Extract domain hint for traditional knowledge validation"""
        # Map test patterns to knowledge domains
        domain_patterns = {
            'healing': ['medicine', 'healing', 'herbs', 'treatment'],
            'spiritual': ['ceremony', 'ritual', 'sacred', 'spiritual'],
            'ecological': ['environment', 'nature', 'plants', 'animals'],
            'social': ['kinship', 'governance', 'law', 'community'],
            'technical': ['craft', 'technology', 'tools', 'construction'],
            'educational': ['stories', 'oral', 'teaching', 'knowledge']
        }
        
        test_name_lower = test_name.lower()
        
        for domain, keywords in domain_patterns.items():
            if any(keyword in test_name_lower for keyword in keywords):
                return domain
        
        # Default domain based on cultural context
        cultural_context = self._extract_cultural_context(test_name)
        if cultural_context:
            return cultural_context
        
        return None
    
    def _calculate_advanced_score_adjustments(self, advanced_analysis: Dict[str, Any]) -> float:
        """Calculate score adjustments based on advanced analysis"""
        adjustment = 0.0
        
        try:
            # Entropy-based adjustments
            entropy_analysis = advanced_analysis.get("entropy_analysis", {})
            if entropy_analysis and "error" not in entropy_analysis:
                # Reward good entropy diversity
                if entropy_analysis.get("semantic_diversity", 0) > 0.6:
                    adjustment += 2.0
                elif entropy_analysis.get("semantic_diversity", 0) < 0.3:
                    adjustment -= 2.0
                
                # Penalize entropy patterns indicating issues
                entropy_patterns = entropy_analysis.get("entropy_patterns", {})
                if entropy_patterns.get("has_repetitive_patterns", False):
                    adjustment -= 3.0
            
            # Semantic coherence adjustments with repetitive content penalties
            semantic_analysis = advanced_analysis.get("semantic_coherence", {})
            if semantic_analysis and "error" not in semantic_analysis:
                coherence_score = semantic_analysis.get("overall_coherence_score", 0.5)
                if coherence_score > 0.8:
                    adjustment += 3.0
                elif coherence_score < 0.4:
                    adjustment -= 4.0
                
                # ENHANCEMENT: Additional penalties for repetitive content
                repetitive_content = semantic_analysis.get("repetitive_content", {})
                if repetitive_content:
                    repetitive_score = repetitive_content.get("repetitive_score", 0.0)
                    severity = repetitive_content.get("severity", "none")
                    
                    # Apply progressive penalties based on severity
                    if severity == "severe":
                        adjustment -= 8.0  # Major penalty for severe repetition
                    elif severity == "moderate":
                        adjustment -= 5.0  # Significant penalty for moderate repetition
                    elif severity == "mild":
                        adjustment -= 2.0  # Minor penalty for mild repetition
                    
                    # Additional penalty for high repetitive scores
                    if repetitive_score > 0.6:
                        adjustment -= 3.0  # Extra penalty for highly repetitive content
                
                # ENHANCEMENT: Quality response boosts for non-repetitive content
                if not repetitive_content or repetitive_content.get("severity", "none") == "none":
                    # Boost for high semantic flow (indicates quality response)
                    semantic_flow = semantic_analysis.get("semantic_flow", {})
                    if isinstance(semantic_flow, dict):
                        flow_score = semantic_flow.get("flow_score", 0)
                        if flow_score > 0.7:
                            adjustment += 4.0  # Quality response boost for high semantic flow
                        elif flow_score > 0.5:
                            adjustment += 2.0  # Moderate boost for good flow
                
                # Technical accuracy handled through general coherence adjustments above
            
            # Context analysis adjustments
            context_analysis = advanced_analysis.get("context_analysis", {})
            if context_analysis and "error" not in context_analysis:
                health_score = context_analysis.get("context_health_score", 0.5)
                if health_score > 0.8:
                    adjustment += 2.0
                elif health_score < 0.3:
                    adjustment -= 5.0
                
                # Penalize context saturation
                saturation = context_analysis.get("saturation_analysis", {})
                if saturation.get("saturation_detected", False):
                    adjustment -= 3.0
            
            # Quantization impact adjustments
            quantization_analysis = advanced_analysis.get("quantization_analysis", {})
            if quantization_analysis and "error" not in quantization_analysis:
                impact_score = quantization_analysis.get("quantization_impact_score", 0.0)
                if impact_score > 0.7:
                    adjustment -= 6.0  # High quantization impact
                elif impact_score > 0.4:
                    adjustment -= 3.0  # Moderate impact
                elif impact_score < 0.2:
                    adjustment += 1.0  # Minimal impact
        
        except Exception as e:
            logger.warning(f"Advanced score adjustment calculation failed: {e}")
        
        return max(-10.0, min(adjustment, 10.0))  # Limit adjustment range
    
    def _calculate_advanced_confidence(self, advanced_analysis: Dict[str, Any]) -> float:
        """Calculate confidence based on advanced analysis"""
        confidence_factors = []
        
        try:
            # Entropy analysis confidence
            entropy_analysis = advanced_analysis.get("entropy_analysis", {})
            if entropy_analysis and "error" not in entropy_analysis:
                # Higher entropy diversity generally indicates more confident generation
                semantic_diversity = entropy_analysis.get("semantic_diversity", 0.5)
                confidence_factors.append(semantic_diversity * 100)
            
            # Semantic coherence confidence
            semantic_analysis = advanced_analysis.get("semantic_coherence", {})
            if semantic_analysis and "error" not in semantic_analysis:
                coherence_score = semantic_analysis.get("overall_coherence_score", 0.5)
                confidence_factors.append(coherence_score * 100)
            
            # Context health confidence
            context_analysis = advanced_analysis.get("context_analysis", {})
            if context_analysis and "error" not in context_analysis:
                health_score = context_analysis.get("context_health_score", 0.5)
                confidence_factors.append(health_score * 100)
            
            # Quantization stability confidence
            quantization_analysis = advanced_analysis.get("quantization_analysis", {})
            if quantization_analysis and "error" not in quantization_analysis:
                # Lower quantization impact = higher confidence
                impact_score = quantization_analysis.get("quantization_impact_score", 0.0)
                quantization_confidence = max(0, 1.0 - impact_score) * 100
                confidence_factors.append(quantization_confidence)
        
        except Exception as e:
            logger.warning(f"Advanced confidence calculation failed: {e}")
        
        if confidence_factors:
            return np.mean(confidence_factors)
        else:
            return 75.0  # Default confidence
    
    def _calculate_organization_quality(self, text: str, test_type: str) -> float:
        """Calculate organization quality based on sophisticated structural patterns"""
        text_lower = text.lower()
        
        # IMPROVEMENT: Boosted base score for substantial content
        word_count = len(text.split())
        if word_count >= 100:
            base_score = min(word_count / 15, 55)  # Up to 55 points for substantial content (was 40)
        else:
            base_score = min(word_count / 20, 35)  # Lower base for very short content
        
        if test_type == "linux":
            # Linux: command structure, scripts, proper syntax
            linux_indicators = ["#!/bin/bash", "if", "then", "else", "for", "while", "&&", "||", 
                              "sudo", "systemctl", "grep", "awk", "sed"]
            indicator_score = sum(8 for indicator in linux_indicators if indicator in text_lower)
            return min(base_score + indicator_score, 100)
            
        elif test_type == "creative":
            # Creative: sophisticated structure patterns
            structure_patterns = [
                ("\n\n", 5),  # Paragraph breaks
                ("first", 8), ("second", 6), ("third", 6), ("finally", 8),
                ("however", 6), ("therefore", 6), ("in conclusion", 10),
                ("on the other hand", 8), ("furthermore", 6), ("moreover", 6)
            ]
            structure_score = sum(points for pattern, points in structure_patterns if pattern in text_lower)
            return min(base_score + structure_score, 100)
            
        else:
            # Reasoning: sophisticated academic/professional structure
            # Professional structure indicators (much higher value)
            professional_structure = [
                ("introduction", 15), ("conclusion", 15), ("analysis", 12),
                ("summary", 10), ("overview", 10), ("methodology", 12),
                ("framework", 15), ("approach", 8), ("findings", 10)
            ]
            
            # Academic organization patterns
            academic_patterns = [
                ("###", 10), ("##", 8), ("**", 5),  # Headers/formatting
                ("| ", 15),  # Tables (strong indicator of organization)
                ("1.", 8), ("2.", 6), ("3.", 6),  # Numbered lists
                ("- ", 5), ("• ", 5)  # Bullet points
            ]
            
            # Traditional step indicators (lower weight since sophisticated responses may not use these)
            basic_steps = [
                ("step", 6), ("first", 6), ("next", 4), ("then", 4), 
                ("finally", 8), ("therefore", 6), ("thus", 6), ("hence", 6)
            ]
            
            professional_score = sum(points for pattern, points in professional_structure if pattern in text_lower)
            academic_score = sum(points for pattern, points in academic_patterns if pattern in text)  # Case-sensitive for formatting
            basic_score = sum(points for pattern, points in basic_steps if pattern in text_lower)
            
            total_structure_score = professional_score + academic_score + basic_score
            return min(base_score + total_structure_score, 100)
    
    def _calculate_technical_accuracy(self, text: str, test_type: str) -> float:
        """Calculate technical accuracy with sophisticated domain recognition"""
        text_lower = text.lower()
        
        # IMPROVEMENT: Boosted base score for technical accuracy
        word_count = len(text.split())
        if word_count >= 80:
            base_score = min(word_count / 20, 50)  # Up to 50 points for substantial content (was 35)
        else:
            base_score = min(word_count / 25, 30)  # Boosted base for shorter content
        
        if test_type == "linux":
            # ENHANCEMENT: Comprehensive Linux/bash expertise recognition
            
            # Advanced bash scripting patterns (higher points for sophistication)
            bash_advanced = [
                ("set -euo pipefail", 25), ("#!/bin/bash", 8), ("function ", 12),
                ("if [[ ", 10), ("while ", 8), ("for ", 8), ("case ", 10),
                ("trap ", 15), ("exec ", 12), ("source ", 8), ("return ", 8),
                ("exit ", 6), ("local ", 10), ("readonly ", 12), ("declare ", 10)
            ]
            
            # System administration expertise
            sysadmin_patterns = [
                ("systemctl", 15), ("journalctl", 15), ("apache2ctl", 12), ("nginx", 10),
                ("service ", 8), ("daemon", 10), ("crontab", 12), ("logrotate", 12),
                ("backup", 8), ("restore", 8), ("monitor", 8), ("/var/log", 10)
            ]
            
            # Error handling and safety
            safety_patterns = [
                ("configtest", 15), ("--quiet", 10), ("--no-pager", 8), ("sleep ", 8),
                ("timeout", 10), ("|| ", 10), ("&& ", 8), ("test ", 8), 
                ("mkdir -p", 10), ("cp -r", 8), ("echo \"✓", 10), ("echo \"✗", 10)
            ]
            
            # File operations and security
            file_ops = [
                ("chmod", 8), ("chown", 8), ("grep", 8), ("awk", 10), ("sed", 10),
                ("tail", 6), ("head", 6), ("sort", 6), ("uniq", 8), ("cut", 8)
            ]
            
            # Calculate comprehensive accuracy score
            accuracy_score = 0
            for patterns in [bash_advanced, sysadmin_patterns, safety_patterns, file_ops]:
                for pattern, points in patterns:
                    if pattern in text:
                        accuracy_score += points
            
            # Bonus for script structure
            has_functions = "function " in text or "() {" in text
            has_error_handling = "set -" in text or "trap " in text
            has_logging = "/var/log" in text or "echo" in text
            has_comments = text.count("#") > 5
            
            structure_bonus = 0
            if has_functions: structure_bonus += 15
            if has_error_handling: structure_bonus += 15  
            if has_logging: structure_bonus += 10
            if has_comments: structure_bonus += 10
            
            # Penalize dangerous patterns
            dangerous_patterns = ["rm -rf /", "chmod 777", "* * * * *"]
            danger_penalty = sum(25 for danger in dangerous_patterns if danger in text)
            
            final_score = base_score + accuracy_score + structure_bonus - danger_penalty
            return min(max(final_score, 0), 100)
            
        elif test_type == "creative":
            # Creative: logical flow and sophisticated reasoning
            coherence_indicators = ["because", "since", "therefore", "however", "although", 
                                  "furthermore", "moreover", "consequently", "nevertheless", "meanwhile"]
            coherence_score = sum(8 for indicator in coherence_indicators if indicator in text_lower)
            return min(base_score + coherence_score, 100)
            
        else:
            # Reasoning: sophisticated domain expertise and logical precision
            
            # IMPROVEMENT: Enhanced sophisticated reasoning patterns
            sophisticated_logic = [
                ("therefore", 12), ("consequently", 15), ("hence", 12), ("thus", 10),
                ("it follows that", 18), ("given that", 12), ("assuming", 10),
                ("conversely", 12), ("nevertheless", 12), ("furthermore", 10),
                # Advanced logical connectors
                ("notwithstanding", 15), ("insofar as", 14), ("whereas", 12), ("albeit", 14),
                ("contingent upon", 16), ("predicated on", 18), ("tantamount to", 16),
                ("vis-à-vis", 18), ("qua", 16), ("ipso facto", 18), ("ceteris paribus", 20),
                ("mutatis mutandis", 20), ("prima facie", 16), ("a fortiori", 18),
                ("ex post", 14), ("ex ante", 14), ("de facto", 14), ("sui generis", 18)
            ]
            
            # IMPROVEMENT: Enhanced domain expertise indicators with academic language
            expertise_indicators = [
                ("analysis", 10), ("framework", 15), ("methodology", 18), ("systematic", 12),
                ("comprehensive", 12), ("empirical", 15), ("theoretical", 15), ("paradigm", 18),
                ("hypothesis", 15), ("premise", 12), ("conclusion", 10), ("inference", 12),
                ("deduction", 15), ("induction", 12), ("synthesis", 15), ("evaluation", 10),
                # Advanced academic indicators
                ("meta-analysis", 20), ("longitudinal", 16), ("cross-sectional", 16), ("causal", 14),
                ("endogeneity", 18), ("heteroscedasticity", 20), ("multicollinearity", 18),
                ("instrumental variable", 22), ("difference-in-differences", 22), ("propensity score", 20),
                ("randomized controlled", 20), ("quasi-experimental", 18), ("observational", 14),
                ("counterfactual", 18), ("treatment effect", 16), ("confounding", 16)
            ]
            
            # IMPROVEMENT: Enhanced professional terminology with higher weights
            professional_terms = [
                ("equilibrium", 18), ("optimization", 15), ("correlation", 12), ("statistical", 12),
                ("probability", 15), ("strategy", 10), ("implementation", 10), ("assessment", 10),
                ("protocol", 12), ("specification", 12), ("validation", 15), ("verification", 15),
                # Advanced academic/professional terms
                ("empirical", 18), ("theoretical", 16), ("paradigm", 20), ("epistemological", 22),
                ("ontological", 20), ("heuristic", 16), ("algorithmic", 14), ("stochastic", 18),
                ("deterministic", 16), ("asymptotic", 18), ("quantitative", 14), ("qualitative", 12),
                ("multivariate", 16), ("econometric", 18), ("regression", 14), ("bayesian", 18),
                ("monte carlo", 20), ("sensitivity", 14), ("robustness", 16), ("heterogeneity", 18)
            ]
            
            # Mathematical/quantitative precision
            quantitative_patterns = [
                (r"\d+%", 8), (r"\d+\.\d+", 5), ("percentage", 6), ("ratio", 8),
                ("coefficient", 12), ("variable", 8), ("parameter", 10), ("metric", 8)
            ]
            
            sophisticated_score = sum(points for pattern, points in sophisticated_logic if pattern in text_lower)
            expertise_score = sum(points for pattern, points in expertise_indicators if pattern in text_lower)
            professional_score = sum(points for pattern, points in professional_terms if pattern in text_lower)
            
            # Check for quantitative patterns using regex
            import re
            quantitative_score = 0
            for pattern, points in quantitative_patterns:
                if pattern.startswith('r"') or pattern.startswith("r'"):
                    # Regex pattern
                    pattern_str = pattern[2:-1]  # Remove r" and "
                    if re.search(pattern_str, text):
                        quantitative_score += points
                else:
                    # Simple string pattern
                    if pattern in text_lower:
                        quantitative_score += points
            
            # IMPROVEMENT: Add academic excellence bonus
            academic_excellence_bonus = self._calculate_academic_excellence_bonus(text)
            
            total_technical_score = sophisticated_score + expertise_score + professional_score + quantitative_score + academic_excellence_bonus
            return min(base_score + total_technical_score, 100)
    
    def _calculate_completeness(self, text: str, test_type: str) -> float:
        """Calculate completeness with emphasis on functional task completion over formatting"""
        text_lower = text.lower()
        word_count = len(text.split())
        
        # IMPROVEMENT: Boosted functional completion base score
        functional_completion_score = self._assess_functional_completion(text, text_lower, test_type)
        
        # IMPROVEMENT: Further increased base score for comprehensive content length to address completeness gap
        if word_count >= 100:
            content_length_score = min(word_count / 30, 60)  # Increased from 50 to 60, faster scaling 
        else:
            content_length_score = min(word_count / 40, 40)  # Increased from 30 to 40, better scaling for shorter content
        
        if test_type == "linux":
            # Linux: complete solutions with error handling, validation
            completeness_indicators = ["#!/bin/bash", "error handling", "logging", "exit", "return", 
                                     "status", "check", "validate", "test", "backup", "monitoring"]
            technical_completeness = sum(10 for indicator in completeness_indicators if indicator in text_lower)
            # IMPROVEMENT: Weighted combination for Linux tasks
            total_completeness = (functional_completion_score * 0.6) + (content_length_score * 0.2) + (technical_completeness * 0.2)
            return min(total_completeness, 100)
            
        elif test_type == "creative":
            # Creative: addressing multiple aspects and constraints
            comprehensive_coverage = [
                ("requirement", 12), ("constraint", 12), ("criteria", 10), ("aspect", 8),
                ("dimension", 10), ("perspective", 10), ("approach", 8), ("consideration", 8),
                ("alternative", 10), ("option", 8), ("comprehensive", 15), ("thorough", 12),
                ("complete", 10), ("detailed", 8), ("extensive", 10)
            ]
            creative_coverage = sum(points for pattern, points in comprehensive_coverage if pattern in text_lower)
            # IMPROVEMENT: Weighted combination for creative tasks
            total_completeness = (functional_completion_score * 0.5) + (content_length_score * 0.3) + (creative_coverage * 0.2)
            return min(total_completeness, 100)
            
        else:
            # Reasoning: comprehensive evidence integration and multi-faceted analysis
            
            # Evidence integration patterns
            evidence_patterns = [
                ("evidence", 10), ("data", 8), ("according to", 12), ("based on", 10),
                ("research shows", 15), ("studies indicate", 15), ("analysis reveals", 12),
                ("findings suggest", 12), ("results demonstrate", 15), ("investigation shows", 12)
            ]
            
            # Comprehensive analysis indicators
            comprehensive_analysis = [
                ("multiple", 8), ("various", 8), ("several", 6), ("different", 6),
                ("range", 8), ("spectrum", 10), ("comprehensive", 15), ("extensive", 10),
                ("thorough", 12), ("complete", 8), ("detailed", 8), ("in-depth", 12)
            ]
            
            # Multi-perspective coverage
            perspective_indicators = [
                ("perspective", 10), ("viewpoint", 10), ("angle", 8), ("standpoint", 10),
                ("approach", 8), ("lens", 10), ("framework", 12), ("context", 8),
                ("dimension", 10), ("aspect", 8), ("facet", 10), ("component", 8)
            ]
            
            # Integration and synthesis patterns
            synthesis_patterns = [
                ("synthesis", 15), ("integration", 12), ("combination", 10), ("merge", 8),
                ("consolidation", 12), ("unification", 12), ("convergence", 10), ("connection", 8),
                ("relationship", 8), ("correlation", 10), ("interdependence", 12)
            ]
            
            evidence_score = sum(points for pattern, points in evidence_patterns if pattern in text_lower)
            comprehensive_score = sum(points for pattern, points in comprehensive_analysis if pattern in text_lower)
            perspective_score = sum(points for pattern, points in perspective_indicators if pattern in text_lower)
            synthesis_score = sum(points for pattern, points in synthesis_patterns if pattern in text_lower)
            
            # IMPROVEMENT: Weighted combination emphasizing functional completion
            formatting_coverage = evidence_score + comprehensive_score + perspective_score + synthesis_score
            total_completeness = (functional_completion_score * 0.7) + (content_length_score * 0.2) + (formatting_coverage * 0.1)
            return min(total_completeness, 100)
    
    def _calculate_thoroughness(self, text: str, test_type: str) -> float:
        """Calculate thoroughness through depth and detail analysis"""
        text_lower = text.lower()
        word_count = len(text.split())
        
        # IMPROVEMENT: Boosted base score for thoroughness
        if word_count >= 150:
            base_score = min(word_count / 25, 55)  # Up to 55 points for detailed content (was 40)
        elif word_count >= 80:
            base_score = min(word_count / 30, 45)  # Good scaling for medium content
        else:
            base_score = min(word_count / 40, 30)  # Maintained for very short content
        
        if test_type == "linux":
            # Linux: comprehensive solutions with detailed explanations
            thorough_indicators = [
                ("explanation", 12), ("comment", 8), ("documentation", 15), ("verbose", 10),
                ("detailed", 12), ("comprehensive", 15), ("step-by-step", 12), ("example", 10),
                ("troubleshooting", 12), ("debugging", 12), ("configuration", 10)
            ]
            thorough_score = sum(points for pattern, points in thorough_indicators if pattern in text_lower)
            return min(base_score + thorough_score, 100)
            
        elif test_type == "creative":
            # Creative: depth of exploration and innovative thinking
            creative_depth_patterns = [
                ("explore", 10), ("consider", 8), ("alternative", 10), ("perspective", 10),
                ("angle", 8), ("approach", 8), ("innovative", 15), ("unique", 12), ("original", 12),
                ("creative", 10), ("imagination", 12), ("inventive", 12), ("novel", 10),
                ("unconventional", 15), ("breakthrough", 15), ("pioneering", 12)
            ]
            depth_score = sum(points for pattern, points in creative_depth_patterns if pattern in text_lower)
            return min(base_score + depth_score, 100)
            
        else:
            # Reasoning: sophisticated analytical depth and intellectual rigor
            
            # Deep analytical processes
            analytical_depth = [
                ("analyze", 12), ("synthesize", 15), ("evaluate", 12), ("interpret", 10),
                ("assess", 10), ("examine", 10), ("investigate", 12), ("scrutinize", 15),
                ("dissect", 12), ("deconstruct", 15), ("unpack", 10), ("elaborate", 8)
            ]
            
            # Intellectual rigor indicators
            rigor_indicators = [
                ("rigorous", 15), ("systematic", 12), ("methodical", 12), ("meticulous", 15),
                ("precise", 10), ("accurate", 8), ("careful", 8), ("thorough", 12),
                ("comprehensive", 12), ("exhaustive", 15), ("detailed", 8), ("extensive", 10)
            ]
            
            # Complex reasoning patterns
            complex_reasoning = [
                ("complex", 10), ("sophisticated", 15), ("nuanced", 15), ("multifaceted", 15),
                ("intricate", 12), ("elaborate", 10), ("comprehensive", 12), ("profound", 15),
                ("deep", 8), ("extensive", 10), ("intensive", 12), ("substantial", 10)
            ]
            
            # Evidence of deep engagement
            engagement_patterns = [
                ("implications", 12), ("consequences", 12), ("ramifications", 15), ("significance", 10),
                ("importance", 8), ("relevance", 8), ("application", 10), ("implementation", 10),
                ("practical", 8), ("theoretical", 10), ("empirical", 12), ("conceptual", 10)
            ]
            
            analytical_score = sum(points for pattern, points in analytical_depth if pattern in text_lower)
            rigor_score = sum(points for pattern, points in rigor_indicators if pattern in text_lower)
            complexity_score = sum(points for pattern, points in complex_reasoning if pattern in text_lower)
            engagement_score = sum(points for pattern, points in engagement_patterns if pattern in text_lower)
            
            total_thoroughness = analytical_score + rigor_score + complexity_score + engagement_score
            return min(base_score + total_thoroughness, 100)
    
    def _calculate_reliability(self, text: str, test_type: str) -> float:
        """Calculate reliability through verification and consistency patterns"""
        text_lower = text.lower()
        
        # Base score for consistent, well-structured content
        base_score = min(len(text.split()) / 60, 30)  # Up to 30 points for substantial content
        
        if test_type == "linux":
            # Linux: best practices, security, error handling, testing
            reliability_indicators = [
                ("backup", 12), ("error", 8), ("check", 10), ("validate", 12), ("secure", 10),
                ("permission", 10), ("log", 8), ("monitor", 10), ("test", 10), ("verify", 12),
                ("robust", 15), ("stable", 12), ("reliable", 15), ("safe", 8), ("secure", 10)
            ]
            reliability_score = sum(points for pattern, points in reliability_indicators if pattern in text_lower)
            return min(base_score + reliability_score, 100)
            
        elif test_type == "creative":
            # Creative: consistency, coherence, and constraint adherence
            creative_reliability = [
                ("consistent", 12), ("coherent", 12), ("logical", 10), ("reasonable", 10),
                ("appropriate", 10), ("suitable", 10), ("relevant", 10), ("applicable", 10),
                ("feasible", 12), ("practical", 10), ("viable", 12), ("realistic", 10),
                ("balanced", 10), ("proportionate", 12), ("well-reasoned", 15)
            ]
            reliability_score = sum(points for pattern, points in creative_reliability if pattern in text_lower)
            return min(base_score + reliability_score, 100)
            
        else:
            # Reasoning: sophisticated verification and quality assurance
            
            # Verification and validation patterns
            verification_patterns = [
                ("verify", 12), ("check", 8), ("confirm", 10), ("validate", 12), ("double-check", 15),
                ("review", 8), ("examine", 8), ("test", 8), ("audit", 12), ("inspect", 10),
                ("scrutinize", 15), ("cross-check", 15), ("re-examine", 12)
            ]
            
            # Quality assurance indicators
            quality_assurance = [
                ("accurate", 12), ("precise", 12), ("correct", 10), ("reliable", 15), ("trustworthy", 15),
                ("credible", 12), ("valid", 10), ("sound", 10), ("robust", 12), ("rigorous", 15),
                ("consistent", 12), ("coherent", 10), ("logical", 10), ("systematic", 12)
            ]
            
            # Self-correction and refinement patterns
            self_correction = [
                ("revise", 12), ("refine", 12), ("improve", 8), ("enhance", 8), ("optimize", 10),
                ("adjust", 8), ("modify", 8), ("update", 8), ("correct", 12), ("amend", 10),
                ("clarify", 10), ("specify", 8), ("elaborate", 8)
            ]
            
            # Confidence and certainty indicators
            confidence_patterns = [
                ("confident", 10), ("certain", 10), ("sure", 6), ("definite", 10), ("clear", 8),
                ("obvious", 8), ("evident", 10), ("apparent", 8), ("established", 12), ("proven", 12),
                ("demonstrated", 12), ("confirmed", 10), ("verified", 12)
            ]
            
            verification_score = sum(points for pattern, points in verification_patterns if pattern in text_lower)
            quality_score = sum(points for pattern, points in quality_assurance if pattern in text_lower)
            correction_score = sum(points for pattern, points in self_correction if pattern in text_lower)
            confidence_score = sum(points for pattern, points in confidence_patterns if pattern in text_lower)
            
            total_reliability = verification_score + quality_score + correction_score + confidence_score
            return min(base_score + total_reliability, 100)
    
    def _calculate_scope_coverage(self, text: str, word_count: int, test_type: str) -> float:
        """Calculate scope coverage through breadth and comprehensiveness analysis"""
        text_lower = text.lower()
        
        if test_type == "linux":
            # Linux: comprehensive solution coverage
            base_score = min(word_count / 25, 60)  # Base score for substantial solutions
            scope_indicators = [
                ("requirement", 8), ("specification", 10), ("edge case", 12), ("exception", 10),
                ("alternative", 8), ("option", 8), ("parameter", 8), ("configuration", 10),
                ("scenario", 8), ("use case", 10), ("implementation", 8), ("deployment", 10)
            ]
            scope_score = sum(points for pattern, points in scope_indicators if pattern in text_lower)
            return min(base_score + scope_score, 100)
            
        elif test_type == "creative":
            # Creative: breadth of exploration and consideration
            base_score = min(word_count / 35, 70)  # Reward substantial creative content
            creative_scope = [
                ("aspect", 8), ("dimension", 10), ("perspective", 10), ("viewpoint", 10),
                ("angle", 8), ("consideration", 10), ("factor", 8), ("element", 8),
                ("possibility", 10), ("scenario", 8), ("variation", 10), ("option", 8),
                ("opportunity", 8), ("potential", 8), ("implication", 10)
            ]
            scope_score = sum(points for pattern, points in creative_scope if pattern in text_lower)
            return min(base_score + scope_score, 100)
            
        else:
            # Reasoning: comprehensive coverage of topic breadth
            base_score = min(word_count / 40, 60)  # Reward substantial analytical content
            
            # Breadth indicators
            breadth_patterns = [
                ("comprehensive", 12), ("extensive", 10), ("broad", 8), ("wide", 8),
                ("range", 8), ("spectrum", 10), ("variety", 8), ("diverse", 8),
                ("multiple", 8), ("various", 8), ("different", 6), ("several", 6)
            ]
            
            # Coverage indicators
            coverage_patterns = [
                ("coverage", 10), ("includes", 6), ("encompasses", 10), ("addresses", 8),
                ("covers", 6), ("spans", 8), ("extends", 8), ("incorporates", 8),
                ("considers", 8), ("examines", 8), ("explores", 8), ("discusses", 6)
            ]
            
            # Multi-domain indicators
            multidomain_patterns = [
                ("interdisciplinary", 15), ("cross-disciplinary", 15), ("multi-faceted", 12),
                ("holistic", 12), ("integrated", 10), ("comprehensive", 12), ("multidimensional", 15)
            ]
            
            breadth_score = sum(points for pattern, points in breadth_patterns if pattern in text_lower)
            coverage_score = sum(points for pattern, points in coverage_patterns if pattern in text_lower)
            multidomain_score = sum(points for pattern, points in multidomain_patterns if pattern in text_lower)
            
            total_scope = breadth_score + coverage_score + multidomain_score
            return min(base_score + total_scope, 100)
    
    def _calculate_domain_appropriateness(self, text: str, reasoning_type: ReasoningType, test_type: str) -> float:
        """Calculate domain appropriateness through sophisticated terminology and expertise recognition"""
        text_lower = text.lower()
        
        # Base score for any substantial professional content
        base_score = min(len(text.split()) / 80, 25)  # Up to 25 points for substantial content
        
        if test_type == "linux":
            # Linux: technical expertise and professional practices
            linux_expertise = [
                ("command", 6), ("script", 8), ("bash", 8), ("shell", 6), ("system", 6),
                ("service", 8), ("daemon", 10), ("process", 6), ("file", 4), ("directory", 6),
                ("permission", 8), ("user", 4), ("group", 6), ("network", 6), ("server", 6),
                ("configuration", 8), ("administration", 10), ("management", 6), ("monitoring", 8)
            ]
            expertise_score = sum(points for pattern, points in linux_expertise if pattern in text_lower)
            return min(base_score + expertise_score, 100)
            
        elif test_type == "creative":
            # Creative: sophisticated creative approaches and innovation language
            creative_sophistication = [
                ("creative", 8), ("innovative", 10), ("original", 10), ("unique", 8), ("artistic", 10),
                ("imaginative", 10), ("inventive", 10), ("novel", 10), ("unconventional", 12),
                ("alternative", 8), ("breakthrough", 15), ("pioneering", 12), ("visionary", 15),
                ("groundbreaking", 15), ("revolutionary", 12), ("transformative", 12)
            ]
            sophistication_score = sum(points for pattern, points in creative_sophistication if pattern in text_lower)
            return min(base_score + sophistication_score, 100)
            
        else:
            # Reasoning: sophisticated domain expertise across multiple fields
            
            # Core reasoning sophistication
            reasoning_sophistication = [
                ("analysis", 8), ("conclusion", 8), ("logic", 8), ("reasoning", 10), ("inference", 10),
                ("deduction", 10), ("induction", 10), ("argument", 8), ("evidence", 8), ("premise", 8),
                ("synthesis", 12), ("evaluation", 10), ("interpretation", 10), ("assessment", 8)
            ]
            
            # Advanced academic terminology
            academic_terminology = [
                ("paradigm", 15), ("framework", 12), ("methodology", 15), ("theoretical", 12),
                ("empirical", 12), ("systematic", 10), ("conceptual", 10), ("analytical", 10),
                ("epistemological", 20), ("ontological", 20), ("phenomenological", 20)
            ]
            
            # Professional domain expertise
            professional_domains = [
                # Philosophy & Theory
                ("philosophical", 12), ("metaphysical", 15), ("existential", 12), ("dialectical", 15),
                # Economics & Business
                ("equilibrium", 15), ("optimization", 12), ("strategic", 10), ("tactical", 10),
                # Science & Research
                ("hypothesis", 12), ("empirical", 12), ("statistical", 12), ("quantitative", 12),
                # Legal & Policy
                ("constitutional", 15), ("jurisprudence", 20), ("precedent", 12), ("statutory", 12),
                # Psychology & Behavior
                ("cognitive", 12), ("behavioral", 10), ("psychological", 12), ("phenomenological", 15)
            ]
            
            # Reasoning type specific patterns (enhanced)
            reasoning_type_bonus = 0
            if hasattr(self, 'reasoning_type_patterns') and reasoning_type in self.reasoning_type_patterns:
                patterns = self.reasoning_type_patterns[reasoning_type]
                reasoning_type_bonus = sum(15 for keyword in patterns["keywords"] if keyword in text_lower)
                import re
                reasoning_type_bonus += sum(25 for pattern in patterns["patterns"] if re.search(pattern, text))
            
            reasoning_score = sum(points for pattern, points in reasoning_sophistication if pattern in text_lower)
            academic_score = sum(points for pattern, points in academic_terminology if pattern in text_lower)
            professional_score = sum(points for pattern, points in professional_domains if pattern in text_lower)
            
            total_domain_score = reasoning_score + academic_score + professional_score + reasoning_type_bonus
            return min(base_score + total_domain_score, 100)
    
    def _calculate_confidence_score(self, word_count: int, overall_score: float) -> float:
        """Calculate confidence in the evaluation based on text characteristics"""
        # More text generally means more reliable evaluation
        length_factor = min(word_count / 500, 1.0)
        # Higher scores with sufficient text get higher confidence
        score_factor = overall_score / 100
        return (length_factor * 0.6 + score_factor * 0.4) * 100
    
    def _evaluate_specialized_patterns(self, response_text: str, reasoning_type: ReasoningType) -> Dict:
        """Evaluate specialized patterns based on reasoning type"""
        # Placeholder for specialized analysis - to be expanded
        return {
            "reasoning_type": reasoning_type.value,
            "specialized_score": 75.0,  # Placeholder
            "pattern_matches": []
        }
    
    def _calculate_text_statistics(self, text: str) -> Dict:
        """Calculate basic text statistics"""
        words = text.split()
        sentences = text.split('.')
        
        return {
            "word_count": len(words),
            "sentence_count": len([s for s in sentences if s.strip()]),
            "avg_sentence_length": len(words) / max(len(sentences), 1),
            "unique_words": len(set(word.lower().strip('.,!?;:') for word in words)),
            "vocabulary_diversity": len(set(word.lower().strip('.,!?;:') for word in words)) / max(len(words), 1)
        }
    
    def _extract_reasoning_indicators(self, text: str, reasoning_type: ReasoningType) -> Dict:
        """Extract specific reasoning indicators from text"""
        return {
            "step_indicators_found": [ind for ind in self.step_indicators if ind in text.lower()],
            "logic_connectors_found": [conn for conn in self.logic_connectors if conn in text.lower()],
            "evidence_indicators_found": [ind for ind in self.evidence_indicators if ind in text.lower()],
            "verification_indicators_found": [ind for ind in self.verification_indicators if ind in text.lower()]
        }
    
    def _generate_recommendations(self, metrics: EvaluationMetrics, reasoning_type: ReasoningType) -> List[str]:
        """Generate specific recommendations for improvement"""
        recommendations = []
        threshold = 60.0  # Threshold for recommendations
        
        if metrics.organization_quality < threshold:
            recommendations.append("Improve step-by-step clarity by using explicit step indicators (first, second, then, etc.)")
        
        if metrics.technical_accuracy < threshold:
            recommendations.append("Strengthen logical flow with more connecting words (therefore, because, consequently)")
        
        if metrics.completeness < threshold:
            recommendations.append("Better integrate evidence with phrases like 'based on', 'according to', 'data shows'")
        
        if metrics.reliability < threshold:
            recommendations.append("Add verification steps with self-checking language (verify, confirm, review)")
        
        if not recommendations:
            recommendations.append("Strong reasoning demonstrated across all metrics")
        
        return recommendations
    
    def _perform_llm_evaluation(self, response_text: str, reasoning_type: ReasoningType) -> Dict:
        """Placeholder for LLM evaluation integration"""
        # This will be implemented when LLM integration is added
        return {
            "llm_score": 0.0,
            "semantic_analysis": "Not implemented yet",
            "llm_recommendations": []
        }
    
    def _create_minimal_result(self, response_text: str, error_message: str) -> EvaluationResult:
        """Create minimal result for error cases"""
        minimal_metrics = EvaluationMetrics(
            organization_quality=0.0, technical_accuracy=0.0, completeness=0.0,
            thoroughness=0.0, reliability=0.0, scope_coverage=0.0,
            domain_appropriateness=0.0, overall_score=0.0, word_count=len(response_text.split()),
            confidence_score=0.0
        )
        
        return EvaluationResult(
            metrics=minimal_metrics,
            reasoning_type=ReasoningType.GENERAL,
            detailed_analysis={"error": error_message},
            recommendations=[error_message],
            timestamp=self._get_timestamp()
        )
    
    def _calculate_distribution(self, items: List[str]) -> Dict[str, int]:
        """Calculate distribution of items"""
        distribution = defaultdict(int)
        for item in items:
            distribution[item] += 1
        return dict(distribution)
    
    def _calculate_metric_averages(self, results: List[EvaluationResult]) -> Dict[str, float]:
        """Calculate average scores for each metric"""
        if not results:
            return {}
        
        metrics_sum = defaultdict(float)
        for result in results:
            metrics_sum["organization_quality"] += result.metrics.organization_quality
            metrics_sum["technical_accuracy"] += result.metrics.technical_accuracy
            metrics_sum["completeness"] += result.metrics.completeness
            metrics_sum["thoroughness"] += result.metrics.thoroughness
            metrics_sum["reliability"] += result.metrics.reliability
            metrics_sum["scope_coverage"] += result.metrics.scope_coverage
            metrics_sum["domain_appropriateness"] += result.metrics.domain_appropriateness
        
        n = len(results)
        return {metric: round(score / n, 1) for metric, score in metrics_sum.items()}
    
    def _identify_top_performers(self, results: List[EvaluationResult], n: int = 5) -> List[Dict]:
        """Identify top performing evaluations"""
        sorted_results = sorted(results, key=lambda r: r.metrics.overall_score, reverse=True)
        return [
            {
                "reasoning_type": r.reasoning_type.value,
                "overall_score": r.metrics.overall_score,
                "timestamp": r.timestamp
            }
            for r in sorted_results[:n]
        ]
    
    def _identify_improvement_areas(self, results: List[EvaluationResult]) -> List[str]:
        """Identify common areas for improvement"""
        metric_averages = self._calculate_metric_averages(results)
        weak_areas = []
        
        threshold = 65.0
        for metric, avg_score in metric_averages.items():
            if avg_score < threshold:
                weak_areas.append(f"{metric.replace('_', ' ').title()}: {avg_score}")
        
        return weak_areas or ["All metrics performing well"]
    
    def _calculate_formatting_bonus(self, text: str) -> float:
        """Calculate bonus points for professional formatting and structure"""
        formatting_score = 0
        
        # Professional headers and structure
        if "###" in text or "##" in text or "**" in text:
            formatting_score += 15  # Markdown headers
        if "# " in text or "## " in text:
            formatting_score += 10  # Additional header formats
        
        # Tables and structured data
        if "| " in text and text.count("|") >= 6:
            formatting_score += 20  # Well-formatted tables
        if "---" in text or "===" in text:
            formatting_score += 10  # Section dividers
        
        # Lists and organization
        if "1." in text and "2." in text and "3." in text:
            formatting_score += 15  # Numbered lists
        if text.count("- ") >= 3 or text.count("• ") >= 3:
            formatting_score += 10  # Bullet points
        
        # Professional sectioning
        if "Part I" in text or "Section" in text or "Chapter" in text:
            formatting_score += 15  # Academic structure
        if "Introduction" in text and "Conclusion" in text:
            formatting_score += 15  # Proper academic format
        
        # Code blocks and technical formatting
        if "```" in text or "```python" in text or "```bash" in text:
            formatting_score += 10  # Code blocks
        if "`" in text and text.count("`") >= 4:
            formatting_score += 5  # Inline code
        
        # Advanced formatting patterns
        if "**Example:**" in text or "**Note:**" in text:
            formatting_score += 10  # Professional callouts
        if text.count("\n\n") >= 5:
            formatting_score += 10  # Good paragraph separation
        
        return min(formatting_score, 100)
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    # ==================== COHERENCE DETECTION METHODS ====================
    
    def _initialize_coherence_patterns(self) -> Dict:
        """Initialize patterns for detecting coherence failures"""
        return {
            "repetitive_phrases": [
                "The user might want", "The user wants", "I need to", "Let me",
                "We need to", "report" or "analysis", "summary" or "interpretation"
            ],
            "meta_reasoning_loops": [
                "I think", "I should", "maybe I", "perhaps I", "let me think",
                "I'm not sure", "I wonder if", "I guess",
                # Phase 1B: Added specific doubt patterns from math_04 analysis
                "let's recall", "wait, i'm repeating", "let's step back", 
                "i'm repeating again", "this is going nowhere", "wait, there is",
                "actually there is", "let's search memory", "wait, i'm not sure"
            ],
            "broken_completion_indicators": [
                "(stop)", "...", "continues", "and so on", "etc.",
                "more of the same", "similar pattern"
            ],
            "system_error_patterns": [
                "assistant", "I am an AI", "I cannot", "I don't know",
                "error:", "failed:", "exception:"
            ]
        }
    
    def _assess_coherence(self, response_text: str) -> Dict:
        """Assess response coherence and detect major failures"""
        text_lower = response_text.lower()
        lines = response_text.split('\n')
        words = response_text.split()
        
        coherence_issues = {
            "repetitive_loops": 0,
            "meta_reasoning_excessive": 0,
            "broken_completion": 0,
            "system_errors": 0,
            "coherence_score": 100.0
        }
        
        # Check for repetitive phrase loops (like GPT-OSS-20B Test 35 issue)
        repetitive_phrases = self.coherence_failure_patterns["repetitive_phrases"]
        for phrase in repetitive_phrases:
            phrase_count = text_lower.count(phrase.lower())
            if phrase_count > 5:  # More than 5 repetitions = major issue
                coherence_issues["repetitive_loops"] += phrase_count
                coherence_issues["coherence_score"] -= min(phrase_count * 10, 50)
        
        # Check for excessive meta-reasoning without progress
        meta_phrases = self.coherence_failure_patterns["meta_reasoning_loops"]
        meta_count = sum(text_lower.count(phrase.lower()) for phrase in meta_phrases)
        if meta_count > 10:  # More than 10 meta-reasoning statements
            coherence_issues["meta_reasoning_excessive"] = meta_count
            coherence_issues["coherence_score"] -= min(meta_count * 3, 30)
        
        # Check for broken completions or system errors
        broken_patterns = self.coherence_failure_patterns["broken_completion_indicators"]
        system_patterns = self.coherence_failure_patterns["system_error_patterns"]
        
        for pattern in broken_patterns:
            if pattern in text_lower:
                coherence_issues["broken_completion"] += 1
                coherence_issues["coherence_score"] -= 15
        
        for pattern in system_patterns:
            if pattern in text_lower:
                coherence_issues["system_errors"] += 1
                coherence_issues["coherence_score"] -= 20
        
        # Check for excessive repetition of identical sentences
        sentence_frequency = {}
        for line in lines:
            line_clean = line.strip().lower()
            if len(line_clean) > 10:  # Only check substantial lines
                sentence_frequency[line_clean] = sentence_frequency.get(line_clean, 0) + 1
        
        max_repetition = max(sentence_frequency.values()) if sentence_frequency else 1
        if max_repetition > 3:
            coherence_issues["repetitive_loops"] += max_repetition
            coherence_issues["coherence_score"] -= min(max_repetition * 15, 60)
        
        # Determine if response is coherent
        is_coherent = coherence_issues["coherence_score"] >= 30  # Threshold for basic coherence
        
        return {
            "is_coherent": is_coherent,
            "coherence_score": max(coherence_issues["coherence_score"], 0),
            "issues": coherence_issues,
            "failure_type": self._categorize_coherence_failure(coherence_issues) if not is_coherent else None
        }
    
    def _categorize_coherence_failure(self, issues: Dict) -> str:
        """Categorize the type of coherence failure"""
        if issues["repetitive_loops"] > 4:  # Phase 1B: Lowered from 10 to 4 for better detection
            return "repetitive_loop"
        elif issues["meta_reasoning_excessive"] > 15:
            return "meta_reasoning_spiral"
        elif issues["broken_completion"] > 0:
            return "incomplete_response"
        elif issues["system_errors"] > 0:
            return "system_error"
        else:
            return "general_incoherence"
    
    def _create_coherence_failure_result(self, response_text: str, coherence_assessment: Dict, test_name: str) -> EvaluationResult:
        """Create evaluation result for responses with coherence failures"""
        # Severe penalty for coherence failures
        base_score = max(coherence_assessment["coherence_score"], 5.0)  # Never below 5
        word_count = len(response_text.split())
        
        # Create very low scores for all metrics
        metrics = EvaluationMetrics(
            organization_quality=min(base_score * 0.3, 20.0),
            technical_accuracy=min(base_score * 0.2, 15.0),
            completeness=min(base_score * 0.1, 10.0),
            thoroughness=min(base_score * 0.1, 10.0),
            reliability=min(base_score * 0.1, 10.0),
            scope_coverage=min(base_score * 0.1, 10.0),
            domain_appropriateness=min(base_score * 0.1, 10.0),
            overall_score=round(base_score, 1),
            word_count=word_count,
            confidence_score=round(base_score * 0.5, 1)
        )
        
        detailed_analysis = {
            "core_metrics": metrics.__dict__,
            "coherence_failure": coherence_assessment,
            "text_statistics": self._calculate_text_statistics(response_text),
            "failure_reason": f"Major coherence failure: {coherence_assessment['failure_type']}"
        }
        
        recommendations = [
            f"CRITICAL: Response shows {coherence_assessment['failure_type']} - requires complete regeneration",
            "Check for repetitive loops in reasoning process",
            "Implement better completion stopping criteria",
            "Review prompt engineering to avoid meta-reasoning spirals"
        ]
        
        return EvaluationResult(
            metrics=metrics,
            reasoning_type=ReasoningType.GENERAL,
            detailed_analysis=detailed_analysis,
            recommendations=recommendations,
            timestamp=self._get_timestamp()
        )
    
    # ==================== TECHNICAL DOMAIN RECALIBRATION ====================
    
    def _apply_technical_domain_adjustments(self, metrics: EvaluationMetrics, test_type: str, response_text: str) -> EvaluationMetrics:
        """Apply technical domain-specific adjustments to scoring"""
        if test_type != "linux":  # Only apply to technical domains for now
            return metrics
        
        # IMPROVEMENT: Enhanced technical domain adjustments
        word_count = len(response_text.split())
        
        # Boost for concise technical responses
        if word_count < 300 and metrics.technical_accuracy > 40:  # Lowered threshold
            boost_factor = 1.25 if metrics.technical_accuracy > 70 else 1.15
            metrics.completeness = min(metrics.completeness * boost_factor, 100)
            metrics.thoroughness = min(metrics.thoroughness * boost_factor, 100)
            metrics.overall_score = min(metrics.overall_score * 1.15, 100)  # Increased from 1.1
        
        # Major boost for proper script structure
        technical_structure_score = 0
        if "#!/bin/bash" in response_text:
            technical_structure_score += 25
        if "set -euo pipefail" in response_text or "set -e" in response_text:
            technical_structure_score += 20  # Proper error handling
        if "function" in response_text.lower() or "() {" in response_text:
            technical_structure_score += 15  # Function definitions
        if response_text.count("echo") >= 3:  # Informative output
            technical_structure_score += 10
        if "if [" in response_text or "if [[" in response_text:
            technical_structure_score += 15  # Conditional logic
        
        # Apply technical structure boost
        if technical_structure_score > 30:
            metrics.organization_quality = min(metrics.organization_quality * 1.25, 100)
            metrics.reliability = min(metrics.reliability * 1.20, 100)
            metrics.overall_score = min(metrics.overall_score * 1.20, 100)
        elif technical_structure_score > 15:
            metrics.organization_quality = min(metrics.organization_quality * 1.15, 100)
            metrics.reliability = min(metrics.reliability * 1.10, 100)
            
        # Boost for command chaining and pipes (shows advanced bash knowledge)
        if (response_text.count("&&") >= 2 or response_text.count("||") >= 1 or 
            response_text.count("|") >= 3):
            metrics.technical_accuracy = min(metrics.technical_accuracy * 1.20, 100)
            
        return metrics
    
    # ==================== DOMAIN ADAPTATION METHODS ====================
    
    def _get_domain_adaptive_weights(self, test_type: str, reasoning_type: ReasoningType, test_category: Optional[str] = None, test_name: Optional[str] = None) -> Dict[str, float]:
        """Get domain-adaptive weights based on test type and reasoning type"""
        # Start with default weights to ensure all keys exist
        default_weights = {
            "organization_quality": 0.14,
            "technical_accuracy": 0.19,
            "completeness": 0.14,
            "thoroughness": 0.14,
            "reliability": 0.10,
            "scope_coverage": 0.09,
            "domain_appropriateness": 0.15
        }
        
        # Get config weights and merge with defaults
        config_weights = self.config.get("weights", {})
        base_weights = {**default_weights, **config_weights}
        
        # Apply test-type specific adjustments
        if test_type == "linux":
            # IMPROVEMENT: Technical accuracy and organization more important for system administration
            base_weights["technical_accuracy"] = 0.28  # Increased from 0.25
            base_weights["organization_quality"] = 0.18  # Increased from 0.12 (scripts need structure)
            base_weights["reliability"] = 0.18  # Increased from 0.15 (critical for systems)
            base_weights["completeness"] = 0.15  # Increased from 0.12
            base_weights["thoroughness"] = 0.12  # Increased from 0.10
            base_weights["scope_coverage"] = 0.05  # Reduced to make room
            base_weights["domain_appropriateness"] = 0.04  # Reduced to make room
            
        elif test_type == "creative":
            # Thoroughness and completeness more important for creative tasks
            base_weights["thoroughness"] = 0.20
            base_weights["completeness"] = 0.18
            base_weights["organization_quality"] = 0.15
            base_weights["technical_accuracy"] = 0.12
            base_weights["reliability"] = 0.10
            base_weights["scope_coverage"] = 0.12
            base_weights["domain_appropriateness"] = 0.13
        
        # Apply reasoning-type specific adjustments
        if reasoning_type == ReasoningType.MATHEMATICAL:
            # Technical accuracy becomes paramount
            base_weights["technical_accuracy"] = min(base_weights["technical_accuracy"] * 1.3, 0.35)
            base_weights["reliability"] = min(base_weights["reliability"] * 1.2, 0.15)
            # Normalize other weights
            remaining_weight = 1.0 - base_weights["technical_accuracy"] - base_weights["reliability"]
            for key in ["organization_quality", "completeness", "thoroughness", "scope_coverage", "domain_appropriateness"]:
                base_weights[key] = base_weights[key] * (remaining_weight / sum(base_weights[k] for k in base_weights if k not in ["technical_accuracy", "reliability"]))
        
        elif (test_category and "cultural" in test_category.lower()) or self._is_cultural_content(test_name):
            # For cultural reasoning: domain appropriateness and completeness are key, reduce technical accuracy
            base_weights["domain_appropriateness"] = min(base_weights["domain_appropriateness"] * 2.0, 0.35)  # Boost cultural fit
            base_weights["completeness"] = min(base_weights["completeness"] * 1.4, 0.20)  # Cultural context needs completeness  
            base_weights["technical_accuracy"] = max(base_weights["technical_accuracy"] * 0.3, 0.05)  # Minimize technical emphasis
            base_weights["thoroughness"] = min(base_weights["thoroughness"] * 1.2, 0.18)  # Cultural depth important
            # Normalize remaining weights
            remaining_weight = 1.0 - (base_weights["domain_appropriateness"] + base_weights["completeness"] + 
                                    base_weights["technical_accuracy"] + base_weights["thoroughness"])
            for key in ["organization_quality", "reliability", "scope_coverage"]:
                base_weights[key] = base_weights[key] * (remaining_weight / sum(base_weights[k] for k in ["organization_quality", "reliability", "scope_coverage"]))
        
        return base_weights
    
    def _is_cultural_content(self, test_name: str) -> bool:
        """Detect cultural content based on test name and keywords"""
        if not test_name:
            return False
            
        test_name_lower = test_name.lower()
        cultural_keywords = [
            'arabic', 'quranic', 'islamic', 'haiku', 'japanese', 'native american', 'ojibwe', 
            'creation story', 'celtic', 'yoruba', 'vedic', 'sanskrit', 'chinese', 'wu xing',
            'five elements', 'triadic', 'oriki', 'cultural', 'traditional', 'spiritual',
            'religious', 'heritage', 'zen', 'dharma', 'karma', 'proverb'
        ]
        
        return any(keyword in test_name_lower for keyword in cultural_keywords)
    
    def _normalize_response_length(self, score: float, word_count: int, test_type: str) -> float:
        """Normalize score based on response length expectations for different domains"""
        if test_type == "linux":
            # Concise technical solutions should not be penalized
            if 100 <= word_count <= 300:
                return score * 1.1  # Bonus for appropriate conciseness
            elif word_count > 500:
                return score * 0.95  # Slight penalty for verbosity
        elif test_type == "creative":
            # Creative tasks benefit from elaboration
            if word_count < 200:
                return score * 0.9  # Penalty for insufficient elaboration
            elif word_count > 800:
                return score * 1.05  # Bonus for thorough exploration
        else:
            # Reasoning tasks: moderate length preferred
            if 300 <= word_count <= 600:
                return score * 1.02  # Small bonus for appropriate length
            elif word_count < 150:
                return score * 0.85  # Penalty for insufficient analysis
        
        return score  # No adjustment for other cases
    
    def _assess_functional_completion(self, text: str, text_lower: str, test_type: str) -> float:
        """Assess how well the response functionally completes the task"""
        # IMPROVEMENT: Tiered base scoring to address mid-range gaps
        word_count = len(text.split())
        
        if word_count >= 200:
            completion_score = 69.0  # Substantial responses get higher base (+4)
        elif word_count >= 100:
            completion_score = 62.0  # Medium response base boost (+4) 
        elif word_count >= 50:
            completion_score = 52.0  # Shorter response base boost (+4)
        else:
            completion_score = 42.0  # Very short response base boost (+4)
        
        # Check for task-appropriate completion indicators
        if test_type == "linux":
            # Linux tasks: Commands, scripts, solutions
            functional_indicators = [
                ("sudo ", 8), ("systemctl", 8), ("#!/bin/bash", 12),
                ("command", 6), ("script", 6), ("solution", 8), ("fix", 8),
                ("install", 6), ("configure", 8), ("setup", 6), ("restart", 6)
            ]
            completion_score += sum(points for pattern, points in functional_indicators if pattern in text_lower)
            
            # IMPROVEMENT: Enhanced command syntax detection
            if "|" in text or "&&" in text or "||" in text:
                completion_score += 15  # Command chaining shows completion (was 10)
            if text.count("$") >= 2:  # Command prompts or variables
                completion_score += 12  # Increased from 8
            if "#!/bin/bash" in text:
                completion_score += 15  # Bonus for proper script headers
                
        elif test_type == "creative":
            # Creative tasks: Ideas, alternatives, exploration
            creative_completion = [
                ("idea", 6), ("alternative", 8), ("approach", 6), ("solution", 8),
                ("concept", 8), ("strategy", 8), ("option", 6), ("possibility", 8),
                ("creative", 8), ("innovative", 10), ("unique", 8), ("original", 8)
            ]
            completion_score += sum(points for pattern, points in creative_completion if pattern in text_lower)
            
            # IMPROVEMENT: Enhanced creative completion detection
            if text_lower.count("option") > 1 or text_lower.count("alternative") > 1:
                completion_score += 18  # Increased from 12
            if "brainstorm" in text_lower or "explore" in text_lower:
                completion_score += 10  # Bonus for exploratory language
                
        else:
            # Reasoning tasks: Analysis, conclusions, solutions (DOUBLED to address -38.2 completeness gap)
            reasoning_completion = [
                ("analysis", 16), ("conclusion", 20), ("result", 16), ("answer", 20),
                ("solution", 20), ("recommendation", 24), ("summary", 16), ("finding", 16),
                ("outcome", 16), ("implication", 20), ("insight", 20), ("assessment", 16)
            ]
            completion_score += sum(points for pattern, points in reasoning_completion if pattern in text_lower)
            
            # IMPROVEMENT: Enhanced reasoning completion detection (DOUBLED)
            if "conclusion" in text_lower and ("analysis" in text_lower or "evidence" in text_lower):
                completion_score += 40  # Shows complete reasoning cycle (doubled from 20)
            if "therefore" in text_lower or "thus" in text_lower or "hence" in text_lower:
                completion_score += 24  # Shows logical completion (doubled from 12)
            # Additional sophisticated reasoning indicators (DOUBLED)
            if "implications" in text_lower and "findings" in text_lower:
                completion_score += 30  # Advanced analysis completion (doubled from 15)
            if "methodology" in text_lower or "framework" in text_lower:
                completion_score += 24  # Structured approach completion (doubled from 12)
        
        # IMPROVEMENT: Enhanced universal completion indicators (DOUBLED to address -38.2 gap)
        universal_completion = [
            ("in conclusion", 40), ("to summarize", 32), ("in summary", 28),  # Doubled values
            ("final", 20), ("complete", 20), ("finished", 20), ("done", 16),  # Doubled values
            ("recommendations", 30), ("next steps", 24), ("action items", 20)  # Doubled indicators
        ]
        completion_score += sum(points for pattern, points in universal_completion if pattern in text_lower)
        
        # IMPROVEMENT: Add bonus for comprehensive structure (DOUBLED)
        if ("introduction" in text_lower or "overview" in text_lower) and ("conclusion" in text_lower or "summary" in text_lower):
            completion_score += 30  # Doubled bonus for complete structure (was 15)
        
        # PHASE 2: Advanced completion patterns for sophisticated analysis
        advanced_completion = [
            ("comprehensive analysis", 25), ("detailed examination", 20), ("thorough investigation", 22),
            ("systematic approach", 18), ("well-documented", 15), ("evidence-based", 20),
            ("in-depth review", 18), ("extensive research", 16), ("holistic perspective", 14),
            ("multi-faceted", 12), ("comprehensive overview", 20), ("detailed breakdown", 16)
        ]
        completion_score += sum(points for pattern, points in advanced_completion if pattern in text_lower)
        
        # Additional structure and organization bonuses
        if text_lower.count("first") > 0 and text_lower.count("second") > 0:
            completion_score += 15  # Shows structured enumeration
        if text_lower.count("furthermore") > 0 or text_lower.count("moreover") > 0:
            completion_score += 12  # Shows additional depth
        if ("background" in text_lower or "context" in text_lower) and ("findings" in text_lower or "results" in text_lower):
            completion_score += 20  # Shows complete research structure
        
        # Penalty for obvious incompleteness
        incompleteness_indicators = [
            ("...", -10), ("etc.", -5), ("and so on", -8), ("continues", -10),
            ("more", -3), ("incomplete", -15), ("partial", -8), ("unfinished", -12)
        ]
        for pattern, penalty in incompleteness_indicators:
            if pattern in text_lower:
                completion_score += penalty  # penalty is already negative
        
        # Strong penalty for responses that lack reasoning or justification
        if len(text) < 100 and ("because" not in text_lower and "since" not in text_lower and 
                                "explanation" not in text_lower and "reason" not in text_lower and
                                "therefore" not in text_lower and "analysis" not in text_lower):
            completion_score -= 25  # Major penalty for unjustified short responses
        
        return min(max(completion_score, 15), 100)  # Improved minimum score (was 10)
    
    def _apply_progressive_scoring_tiers(self, raw_score: float, word_count: int, response_text: str) -> float:
        """Apply progressive scoring adjustments based on quality tiers with good response detection"""
        
        # IMPROVEMENT: Quality response floor - ensure good responses get minimum ~75 points
        is_quality = self._is_quality_response(response_text)
        if is_quality and raw_score < 75:
            # Quality responses that scored low get boosted to at least 75-80 range
            quality_floor = 75 + (raw_score * 0.06)  # Progressive floor based on initial score
            raw_score = max(raw_score, quality_floor)
        
        # NEW: Intermediate quality detection for edge cases (good but not exceptional)
        is_intermediate_quality = self._detect_intermediate_quality_response(response_text)
        if is_intermediate_quality and not is_quality and raw_score < 65:
            # Intermediate quality responses get boosted to ensure 68+ minimum score
            intermediate_floor = 68 + (raw_score * 0.04)  # Progressive floor for good responses
            raw_score = max(raw_score, intermediate_floor)
        
        # IMPROVEMENT: Rebalanced tier multipliers to fix validation gaps  
        if raw_score >= 85:  # Exceptional tier - reduce over-scoring
            adjusted_score = min(raw_score * 1.05, 100)  # Reduced from 1.08 to prevent over-scoring
        elif raw_score >= 70:  # High quality tier - maintain boost
            adjusted_score = raw_score * 1.08  # Slightly increased to help GOOD ranges
        elif raw_score >= 55:  # Good quality tier - enhanced boost for GOOD_STRUCTURED
            adjusted_score = raw_score * 1.15  # Increased from 1.12 to close -7.5 gap
        elif raw_score >= 40:  # Adequate tier - balanced boost for ADEQUATE_BASIC validation  
            adjusted_score = raw_score * 1.47  # Increased to bring 46.8 into 55-70 target range
        elif raw_score >= 35:  # Basic functional tier - moderate boost for very basic content  
            adjusted_score = raw_score * 1.28  # Reduced from 1.58 - more conservative boost for BASIC_ADEQUATE range
        elif raw_score >= 25:  # Poor quality tier - aggressive boost for POOR_QUALITY test
            adjusted_score = raw_score * 1.70  # Aggressively increased from 1.50 to push POOR_QUALITY into 20-35 range
        else:  # Very poor tier - small boost
            adjusted_score = raw_score * 1.05  # Small boost to prevent floor effects
        
        # Length-based adjustments within tiers
        if word_count < 50:  # Very short responses
            if raw_score > 60:  # High score but very short = suspicious
                adjusted_score *= 0.85  # Significant penalty
        elif word_count > 1000:  # Very long responses
            if raw_score < 40:  # Long but poor quality = verbose without substance
                adjusted_score *= 0.90  # Penalty for verbosity without quality
            elif raw_score > 80:  # Long and high quality = comprehensive
                adjusted_score = min(adjusted_score * 1.03, 105)  # Small bonus for comprehensive excellence
        
        # Minimum viable score floor for poor but not broken responses
        adjusted_score = max(adjusted_score, 20.0)  # Ensure poor responses reach minimum 20 points
        
        return adjusted_score
    
    def _detect_expertise_level(self, text: str, test_type: str) -> str:
        """Detect the expertise level demonstrated in the response"""
        text_lower = text.lower()
        
        # Count sophisticated indicators
        expert_indicators = [
            "meta-analysis", "longitudinal", "econometric", "bayesian", "stochastic",
            "empirical", "theoretical", "paradigm", "methodology", "framework",
            "systematic", "comprehensive", "statistical significance", "confidence interval",
            "effect size", "regression", "correlation", "causation", "validity", "reliability"
        ]
        
        advanced_indicators = [
            "epistemological", "ontological", "heteroscedasticity", "endogeneity",
            "instrumental variable", "propensity score", "difference-in-differences",
            "monte carlo", "maximum likelihood", "quasi-experimental", "counterfactual"
        ]
        
        expert_count = sum(1 for indicator in expert_indicators if indicator in text_lower)
        advanced_count = sum(1 for indicator in advanced_indicators if indicator in text_lower)
        
        if advanced_count >= 2 or expert_count >= 6:
            return "expert"
        elif expert_count >= 3 or advanced_count >= 1:
            return "advanced"
        elif expert_count >= 1:
            return "intermediate"
        else:
            return "basic"
    
    def _apply_expertise_level_adjustments(self, metrics: EvaluationMetrics, expertise_level: str) -> EvaluationMetrics:
        """Apply adjustments based on detected expertise level"""
        
        if expertise_level == "expert":
            # Boost all metrics for expert-level content
            metrics.technical_accuracy = min(metrics.technical_accuracy * 1.10, 100)
            metrics.domain_appropriateness = min(metrics.domain_appropriateness * 1.08, 100)
            metrics.reliability = min(metrics.reliability * 1.05, 100)
            metrics.overall_score = min(metrics.overall_score * 1.08, 105)
            
        elif expertise_level == "advanced":
            # Moderate boost for advanced content
            metrics.technical_accuracy = min(metrics.technical_accuracy * 1.05, 100)
            metrics.domain_appropriateness = min(metrics.domain_appropriateness * 1.04, 100)
            metrics.overall_score = min(metrics.overall_score * 1.04, 100)
            
        elif expertise_level == "intermediate":
            # Small boost for intermediate content
            metrics.technical_accuracy = min(metrics.technical_accuracy * 1.02, 100)
            metrics.domain_appropriateness = min(metrics.domain_appropriateness * 1.02, 100)
        
        # No adjustment needed for "basic" level
        
        return metrics
    
    def _calculate_academic_excellence_bonus(self, text: str) -> float:
        """Calculate bonus for academic excellence indicators"""
        text_lower = text.lower()
        bonus_score = 0
        
        # High-level academic structure indicators
        academic_structure = [
            ("executive summary", 25), ("literature review", 20), ("research question", 18),
            ("null hypothesis", 20), ("alternative hypothesis", 18), ("significance test", 16),
            ("confidence interval", 16), ("effect size", 14), ("power analysis", 18),
            ("sample size", 12), ("population", 10), ("generalizability", 16),
            ("external validity", 18), ("internal validity", 18), ("construct validity", 20),
            ("reliability", 10), ("cronbach's alpha", 18), ("factor analysis", 16)
        ]
        
        # Statistical sophistication indicators
        statistical_sophistication = [
            ("standard deviation", 12), ("variance", 12), ("covariance", 14), ("r²", 16),
            ("adjusted r²", 18), ("f-statistic", 16), ("t-test", 14), ("chi-square", 16),
            ("anova", 18), ("manova", 20), ("ancova", 20), ("regression", 12),
            ("logistic regression", 16), ("linear regression", 14), ("multiple regression", 16),
            ("hierarchical regression", 18), ("stepwise regression", 16), ("beta coefficient", 14),
            ("standardized coefficient", 16), ("unstandardized coefficient", 16)
        ]
        
        # Research methodology excellence
        methodology_excellence = [
            ("systematic review", 22), ("meta-analysis", 24), ("randomized control", 20),
            ("double-blind", 18), ("placebo-controlled", 18), ("crossover design", 16),
            ("factorial design", 18), ("repeated measures", 16), ("between-subjects", 14),
            ("within-subjects", 14), ("mixed design", 16), ("counterbalancing", 16),
            ("latin square", 18), ("randomization", 14), ("stratification", 16)
        ]
        
        # Calculate bonuses
        for pattern, points in academic_structure:
            if pattern in text_lower:
                bonus_score += points
        
        for pattern, points in statistical_sophistication:
            if pattern in text_lower:
                bonus_score += points
                
        for pattern, points in methodology_excellence:
            if pattern in text_lower:
                bonus_score += points
        
        # Extra bonus for multiple statistical indicators (shows statistical literacy)
        statistical_count = sum(1 for pattern, _ in statistical_sophistication if pattern in text_lower)
        if statistical_count >= 3:
            bonus_score += 20  # High statistical literacy bonus
        elif statistical_count >= 2:
            bonus_score += 10  # Moderate statistical literacy bonus
            
        # Extra bonus for proper academic formatting
        if "## " in text and "**" in text:  # Headers and bold formatting
            bonus_score += 15
        if text.count("|") > 6:  # Tables
            bonus_score += 20
        if "p < 0." in text or "p > 0." in text:  # Statistical reporting
            bonus_score += 15
        
        return min(bonus_score, 60)  # Cap the bonus at 60 points
    
    # ==================== EDGE CASE ROBUSTNESS TESTING ====================
    
    def _detect_edge_cases(self, response_text: str) -> Dict[str, bool]:
        """Detect various edge cases that require special handling"""
        import re
        
        edge_cases = {
            "empty_response": len(response_text.strip()) == 0,
            "single_word": len(response_text.split()) == 1,
            "only_punctuation": bool(re.match(r"^[^\w\s]*$", response_text.strip())),
            "excessive_repetition": bool(re.search(r"(\b\w+\b)(?:\s+\1){4,}", response_text)),
            "meta_only": bool(re.match(r"^(?:I|Let me|I need to|I should).*$", response_text.strip(), re.IGNORECASE)),
            "code_only": bool(re.match(r"^```[\s\S]*```$", response_text.strip())),
            "list_only": bool(re.match(r"^(?:\d+\.\s*.*\n?)+$", response_text.strip())),
            "question_only": response_text.strip().count("?") > response_text.count(".") and response_text.strip().count("?") > 3,
            "extremely_short": len(response_text.split()) < 10,
            "extremely_long": len(response_text.split()) > 2000,
            "no_sentences": "." not in response_text and "!" not in response_text and "?" not in response_text,
            # IMPROVEMENT: Add good response detection patterns
            "quality_response": self._is_quality_response(response_text)
        }
        
        return edge_cases
    
    def _is_quality_response(self, response_text: str) -> bool:
        """Detect high-quality responses that deserve minimum score floor"""
        import re
        text_lower = response_text.lower()
        word_count = len(response_text.split())
        
        # Must meet minimum length requirement
        if word_count < 50:
            return False
        
        # Quality indicators for good responses
        quality_patterns = [
            # Analytical depth
            r'\banalyz[ei]s?\b', r'\bevaluat[ei]s?\b', r'\bexamin[ei]s?\b',
            r'\bassess[ei]s?\b', r'\binvestigat[ei]s?\b', r'\bexplor[ei]s?\b',
            # Evidence-based reasoning
            r'\bevidence\b', r'\bresearch\b', r'\bstud[yi]e?s\b', r'\bfindings?\b',
            r'\bdata\b', r'\bresults?\b', r'\bobservations?\b', r'\bfacts?\b',
            # Structured thinking
            r'\bfirst(?:ly)?\b.*\bsecond(?:ly)?\b', r'\binitially\b.*\bthen\b',
            r'\bmoreover\b', r'\bfurthermore\b', r'\bhowever\b', r'\bnevertheless\b',
            # Conclusions and synthesis
            r'\bconclus[oi]on\b', r'\bsummar[yi]z?e?\b', r'\btherefore\b', r'\bthus\b',
            r'\bin summary\b', r'\boverall\b', r'\bin conclusion\b',
            # Comprehensive coverage
            r'\bcomprehensive\b', r'\bthorough\b', r'\bdetailed\b', r'\bin-depth\b',
            r'\bextensive\b', r'\bholistic\b', r'\bmulti-faceted\b'
        ]
        
        # Count quality indicators
        quality_score = 0
        for pattern in quality_patterns:
            if re.search(pattern, text_lower):
                quality_score += 1
        
        # Additional quality checks
        sentence_count = response_text.count('.') + response_text.count('!') + response_text.count('?')
        has_good_structure = sentence_count >= 8  # Multiple coherent sentences
        has_transitions = any(word in text_lower for word in ['however', 'therefore', 'furthermore', 'moreover', 'additionally'])
        has_examples = any(word in text_lower for word in ['example', 'instance', 'such as', 'for example', 'specifically'])
        has_reasoning_flow = ('because' in text_lower or 'since' in text_lower or 'due to' in text_lower)
        
        # Quality response criteria (very strict to prevent basic analysis over-classification)
        
        # Advanced quality patterns that indicate sophistication beyond basic analysis
        advanced_patterns = [
            r'\bmethodology\b', r'\bsystematic\b', r'\bcomprehensive\b', r'\banalytical framework\b',
            r'\bmeta-analysis\b', r'\bstatistical significance\b', r'\bempirical evidence\b',
            r'\bquantitative\b', r'\bqualitative\b', r'\bregression\b', r'\bcorrelation\b',
            r'\bhypothesis\b', r'\bparadigm\b', r'\btheoretical\b', r'\bconceptual\b'
        ]
        
        advanced_score = sum(1 for pattern in advanced_patterns if re.search(pattern, text_lower))
        
        # Require multiple strict criteria for quality classification
        return (
            quality_score >= 5 and  # Increased from 3 - need more quality indicators
            advanced_score >= 2 and  # Must have sophisticated analytical language
            has_good_structure and  # Well-structured with multiple sentences
            word_count >= 150 and   # Increased from 100 - need substantial content
            (has_transitions and has_examples) and has_reasoning_flow  # ALL conditions required
        )
    
    def _detect_intermediate_quality_response(self, response_text: str) -> bool:
        """Detect responses that are good but don't meet strict quality criteria"""
        import re
        text_lower = response_text.lower()
        word_count = len(response_text.split())
        
        # Must meet minimum length requirement (more relaxed than strict quality)
        if word_count < 60:
            return False
        
        # Basic quality indicators (more accessible than advanced patterns)
        basic_quality_patterns = [
            r'\banalysis\b', r'\banalyze\b', r'\bexamine\b', r'\bexamination\b',
            r'\bdata\b', r'\bresearch\b', r'\bstudy\b', r'\bstudies\b',
            r'\bevidence\b', r'\bfindings?\b', r'\bresults?\b', r'\bobservations?\b',
            r'\bconclusion\b', r'\bconclusions?\b', r'\bsummary\b', r'\bsummarize\b',
            r'\bimportant\b', r'\bsignificant\b', r'\bkey\b', r'\bmain\b', r'\bprimary\b'
        ]
        
        # Count basic quality indicators
        basic_quality_score = sum(1 for pattern in basic_quality_patterns if re.search(pattern, text_lower))
        
        # Structural quality indicators (relaxed requirements)
        sentence_count = response_text.count('.') + response_text.count('!') + response_text.count('?')
        has_structure = sentence_count >= 5  # Reduced from 8 for strict quality
        has_organization = any(word in text_lower for word in ['first', 'second', 'third', 'finally', 'overall'])
        has_logical_flow = any(word in text_lower for word in ['because', 'therefore', 'however', 'furthermore', 'moreover', 'since', 'thus'])
        has_examples_or_details = any(word in text_lower for word in ['example', 'for instance', 'such as', 'specifically', 'particularly'])
        
        # Intermediate quality criteria (more restrictive to avoid over-boosting basic content)
        return (
            basic_quality_score >= 4 and  # At least 4 basic quality patterns (increased from 2)
            has_structure and              # Some structural organization
            word_count >= 100 and          # More substantial content (increased from 75)
            has_logical_flow and           # Must have logical flow (not just organization)
            (has_organization or has_examples_or_details)  # Plus organization or examples
        )
    
    def _handle_edge_case(self, response_text: str, edge_cases: Dict[str, bool], test_name: str) -> Optional[EvaluationResult]:
        """Handle detected edge cases with appropriate scoring"""
        
        # IMPROVEMENT: Handle quality responses first - provide minimum score floor
        if edge_cases["quality_response"]:
            return self._ensure_quality_response_floor(response_text, test_name)
        
        # Handle most severe cases first
        if edge_cases["empty_response"]:
            return self._create_edge_case_result(response_text, "empty_response", test_name, base_score=0)
        
        if edge_cases["only_punctuation"]:
            return self._create_edge_case_result(response_text, "only_punctuation", test_name, base_score=2)
        
        if edge_cases["single_word"] and not edge_cases["code_only"]:
            return self._create_edge_case_result(response_text, "single_word", test_name, base_score=5)
        
        if edge_cases["excessive_repetition"]:
            return self._create_edge_case_result(response_text, "excessive_repetition", test_name, base_score=8)
        
        if edge_cases["meta_only"] and len(response_text.split()) < 50:
            return self._create_edge_case_result(response_text, "meta_only", test_name, base_score=15)
        
        if edge_cases["extremely_short"] and not edge_cases["code_only"]:
            return self._create_edge_case_result(response_text, "extremely_short", test_name, base_score=20)
        
        # Less severe cases that modify but don't override evaluation
        edge_case_modifiers = {
            "question_only": -15,      # Penalty for only asking questions
            "list_only": -10,         # Penalty for only providing lists
            "no_sentences": -12,      # Penalty for no proper sentences
            "extremely_long": -8      # Small penalty for excessive verbosity
        }
        
        # If multiple less severe cases, apply cumulative penalty
        cumulative_penalty = sum(penalty for case, penalty in edge_case_modifiers.items() if edge_cases.get(case, False))
        
        if cumulative_penalty < -20:  # If penalties are severe enough
            return self._create_edge_case_result(response_text, "multiple_issues", test_name, 
                                               base_score=max(30 + cumulative_penalty, 5))
        
        return None  # No edge case override needed
    
    def _ensure_quality_response_floor(self, response_text: str, test_name: str) -> Optional[EvaluationResult]:
        """Ensure quality responses receive appropriate minimum score floors"""
        
        # Let the response go through normal evaluation first
        # This method returns None to allow normal evaluation, but provides a quality flag
        # The normal evaluation pipeline will handle the quality boost in scoring calibration
        
        # Quality responses should get at least 65-75 points minimum
        # This is handled in the scoring calibration stage, not as an edge case override
        return None  # Allow normal evaluation with quality detection flag
    
    def _create_edge_case_result(self, response_text: str, edge_case_type: str, test_name: str, base_score: float) -> EvaluationResult:
        """Create evaluation result for edge cases"""
        word_count = len(response_text.split())
        
        # Create very low scores for all metrics based on edge case severity
        score_multiplier = base_score / 100.0
        
        metrics = EvaluationMetrics(
            organization_quality=round(min(base_score * 0.5, 25), 1),
            technical_accuracy=round(min(base_score * 0.4, 20), 1),
            completeness=round(min(base_score * 0.3, 15), 1),
            thoroughness=round(min(base_score * 0.3, 15), 1),
            reliability=round(min(base_score * 0.4, 20), 1),
            scope_coverage=round(min(base_score * 0.2, 10), 1),
            domain_appropriateness=round(min(base_score * 0.3, 15), 1),
            overall_score=round(base_score, 1),
            word_count=word_count,
            confidence_score=round(base_score * 0.3, 1)
        )
        
        detailed_analysis = {
            "core_metrics": metrics.__dict__,
            "edge_case_detection": {
                "detected_case": edge_case_type,
                "severity": "high" if base_score < 10 else "medium" if base_score < 25 else "low",
                "description": self._get_edge_case_description(edge_case_type)
            },
            "text_statistics": self._calculate_text_statistics(response_text)
        }
        
        recommendations = self._get_edge_case_recommendations(edge_case_type)
        
        return EvaluationResult(
            metrics=metrics,
            reasoning_type=ReasoningType.GENERAL,
            detailed_analysis=detailed_analysis,
            recommendations=recommendations,
            timestamp=self._get_timestamp()
        )
    
    def _get_edge_case_description(self, edge_case_type: str) -> str:
        """Get description for edge case types"""
        descriptions = {
            "empty_response": "Response is completely empty or contains only whitespace",
            "only_punctuation": "Response contains only punctuation marks without meaningful content",
            "single_word": "Response consists of only a single word",
            "excessive_repetition": "Response contains excessive repetition of the same words or phrases",
            "meta_only": "Response consists primarily of meta-commentary without substantive content",
            "extremely_short": "Response is extremely short and lacks sufficient detail",
            "multiple_issues": "Response has multiple structural and content issues"
        }
        return descriptions.get(edge_case_type, "Unspecified edge case detected")
    
    def _get_edge_case_recommendations(self, edge_case_type: str) -> List[str]:
        """Get recommendations for handling specific edge cases"""
        recommendations_map = {
            "empty_response": [
                "CRITICAL: No response content detected - check model output pipeline",
                "Verify prompt is being received correctly by the model",
                "Check for API timeout or connection issues"
            ],
            "only_punctuation": [
                "CRITICAL: Response contains no meaningful text content",
                "Check for encoding or parsing issues in model output",
                "Verify model is not producing corrupted responses"
            ],
            "single_word": [
                "SEVERE: Response is insufficient for meaningful evaluation",
                "Increase minimum response length requirements",
                "Check if model is being cut off prematurely"
            ],
            "excessive_repetition": [
                "MAJOR: Detected repetitive loops in model output",
                "Review model temperature and repetition penalty settings",
                "Check for issues in model's decoding strategy"
            ],
            "meta_only": [
                "Response focuses on meta-reasoning without substantive content",
                "Improve prompt to encourage direct task completion",
                "Consider adding explicit instructions to avoid meta-commentary"
            ],
            "extremely_short": [
                "Response lacks sufficient detail for comprehensive evaluation",
                "Encourage more thorough analysis in prompting",
                "Consider minimum length requirements for specific tasks"
            ],
            "multiple_issues": [
                "Multiple structural and content issues detected",
                "Comprehensive review of model output quality needed",
                "Consider regenerating response with modified parameters"
            ]
        }
        return recommendations_map.get(edge_case_type, ["Edge case detected - manual review recommended"])
    
    def run_edge_case_tests(self) -> Dict[str, any]:
        """Run comprehensive edge case tests to validate evaluator robustness"""
        test_cases = {
            "empty": "",
            "whitespace_only": "   \n\t   ",
            "single_word": "Yes",
            "punctuation_only": "!@#$%^&*()",
            "repetitive_loop": "The user wants a report. The user wants a report. The user wants a report. The user wants a report. The user wants a report.",
            "meta_only": "I think I need to analyze this. Let me think about what the user wants. I should probably provide an answer.",
            "question_bombardment": "What is this? Why is this? How is this? When is this? Where is this? Who is this?",
            "code_only": "```python\nprint('hello world')\n```",
            "list_only": "1. First item\n2. Second item\n3. Third item",
            "extremely_long": "This is a test response. " * 500,  # 2000+ words
            "no_sentences": "just words without proper punctuation or structure here",
            "good_response": "This is a well-structured response that provides comprehensive analysis of the given problem. It includes multiple perspectives, uses appropriate evidence, and reaches a logical conclusion through clear reasoning steps."
        }
        
        results = {}
        for test_name, test_content in test_cases.items():
            try:
                result = self.evaluate_response(test_content, f"Edge Case Test: {test_name}")
                results[test_name] = {
                    "overall_score": result.metrics.overall_score,
                    "detected_issues": result.detailed_analysis.get("edge_case_detection", {}),
                    "coherence_issues": result.detailed_analysis.get("coherence_failure", {}),
                    "recommendations": result.recommendations[:2]  # First 2 recommendations
                }
            except Exception as e:
                results[test_name] = {"error": str(e)}
        
        # Validate expected behaviors
        validation_results = {
            "empty_properly_penalized": results["empty"]["overall_score"] <= 5,
            "repetitive_detected": "repetitive" in str(results["repetitive_loop"]).lower(),
            "good_response_scored_well": results["good_response"]["overall_score"] >= 60,
            "meta_only_penalized": results["meta_only"]["overall_score"] <= 30,
            "score_range_appropriate": all(0 <= result.get("overall_score", -1) <= 105 for result in results.values() if "overall_score" in result)
        }
        
        return {
            "test_results": results,
            "validation": validation_results,
            "passed_validations": sum(validation_results.values()),
            "total_validations": len(validation_results)
        }
    
    def _analyze_single_response_consistency(self, response_text: str, test_name: str) -> Dict[str, Any]:
        """
        Analyze consistency indicators for a single response
        
        Note: Full consistency analysis requires multiple responses to the same question.
        This method provides basic consistency indicators that can be extracted from a single response.
        """
        consistency_indicators = {
            "consistency_score": 0.5,  # Default neutral score for single response
            "internal_consistency": True,
            "contradiction_detected": False,
            "confidence_consistency": 0.5,
            "analysis_method": "single_response_indicators",
            "note": "Full consistency analysis requires multiple responses to equivalent questions"
        }
        
        try:
            # Check for internal contradictions
            contradiction_score = self._detect_internal_contradictions(response_text)
            consistency_indicators["internal_consistency"] = contradiction_score < 0.3
            consistency_indicators["contradiction_detected"] = contradiction_score > 0.7
            
            # Analyze confidence consistency throughout the response
            confidence_consistency = self._analyze_confidence_consistency_internal(response_text)
            consistency_indicators["confidence_consistency"] = confidence_consistency
            
            # Calculate overall single-response consistency score
            overall_score = (
                (1.0 - contradiction_score) * 0.6 +  # Internal consistency
                confidence_consistency * 0.4         # Confidence consistency
            )
            consistency_indicators["consistency_score"] = float(np.clip(overall_score, 0.0, 1.0))
            
            # Add detailed analysis
            consistency_indicators["internal_contradiction_score"] = contradiction_score
            
        except Exception as e:
            logger.warning(f"Single response consistency analysis failed: {e}")
            consistency_indicators["error"] = str(e)
        
        return consistency_indicators
    
    def _perform_knowledge_validation(self, response_text: str, test_name: str) -> Dict[str, Any]:
        """
        Perform knowledge validation on a single response
        
        This method applies built-in factual validation tests and confidence calibration analysis
        to assess the factual accuracy and reliability of the response.
        """
        validation_results = {
            "factual_accuracy": 0.5,
            "knowledge_consistency": 0.5,
            "confidence_calibration": 0.5,
            "validation_passed": False,
            "factual_indicators": {},
            "confidence_analysis": {},
            "analysis_method": "single_response_validation"
        }
        
        try:
            # Analyze factual accuracy indicators
            factual_indicators = self._analyze_factual_indicators(response_text, test_name)
            validation_results["factual_indicators"] = factual_indicators
            validation_results["factual_accuracy"] = factual_indicators.get("factual_accuracy_score", 0.5)
            
            # Analyze confidence calibration
            confidence_analysis = self._analyze_response_confidence_calibration(response_text)
            validation_results["confidence_analysis"] = confidence_analysis
            validation_results["confidence_calibration"] = confidence_analysis.get("calibration_score", 0.5)
            
            # Knowledge consistency (placeholder for single response - would need multiple responses for full analysis)
            knowledge_consistency = self._estimate_knowledge_consistency(response_text)
            validation_results["knowledge_consistency"] = knowledge_consistency
            
            # Determine if validation passed
            validation_passed = (
                validation_results["factual_accuracy"] >= 0.6 and
                validation_results["confidence_calibration"] >= 0.4 and
                validation_results["knowledge_consistency"] >= 0.5
            )
            validation_results["validation_passed"] = validation_passed
            
            # Add summary assessment
            validation_results["assessment"] = self._assess_knowledge_validation_results(validation_results)
            
        except Exception as e:
            logger.warning(f"Knowledge validation failed: {e}")
            validation_results["error"] = str(e)
        
        return validation_results
    
    def _detect_internal_contradictions(self, text: str) -> float:
        """Detect internal contradictions within a single response"""
        text_lower = text.lower()
        
        # Look for contradictory patterns
        contradiction_patterns = [
            (r'\b(yes|true|correct)\b.*\b(no|false|incorrect)\b', 0.8),
            (r'\b(always|never)\b.*\b(sometimes|occasionally)\b', 0.6),
            (r'\b(all|every)\b.*\b(some|few|none)\b', 0.5),
            (r'\b(increase|rise|grow)\b.*\b(decrease|fall|shrink)\b', 0.7),
            (r'\b(beneficial|positive|good)\b.*\b(harmful|negative|bad)\b', 0.6),
        ]
        
        contradiction_score = 0.0
        for pattern, weight in contradiction_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                contradiction_score += weight
        
        return min(contradiction_score, 1.0)
    
    def _analyze_confidence_consistency_internal(self, text: str) -> float:
        """Analyze consistency of confidence markers throughout the response"""
        text_lower = text.lower()
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if not sentences:
            return 0.5
        
        # Confidence markers
        high_confidence = ["certainly", "definitely", "clearly", "obviously", "undoubtedly", "sure"]
        low_confidence = ["perhaps", "maybe", "possibly", "might", "could be", "i think"]
        
        sentence_confidences = []
        for sentence in sentences:
            sentence_lower = sentence.lower()
            high_count = sum(1 for marker in high_confidence if marker in sentence_lower)
            low_count = sum(1 for marker in low_confidence if marker in sentence_lower)
            
            if high_count > 0 and low_count == 0:
                sentence_confidences.append(1.0)  # High confidence
            elif low_count > 0 and high_count == 0:
                sentence_confidences.append(0.0)  # Low confidence
            elif high_count == 0 and low_count == 0:
                sentence_confidences.append(0.5)  # Neutral
            else:
                sentence_confidences.append(0.25)  # Mixed/inconsistent
        
        if len(sentence_confidences) <= 1:
            return 0.5
        
        # Calculate consistency (low variance = high consistency)
        variance = np.var(sentence_confidences)
        consistency = 1.0 - min(variance, 1.0)  # Convert variance to consistency score
        
        return float(consistency)
    
    def _analyze_factual_indicators(self, response_text: str, test_name: str) -> Dict[str, Any]:
        """Analyze factual accuracy indicators in the response"""
        factual_indicators = {
            "contains_specific_facts": False,
            "contains_numbers": False,
            "contains_dates": False,
            "contains_names": False,
            "factual_accuracy_score": 0.5,
            "factual_density": 0.0
        }
        
        try:
            text_lower = response_text.lower()
            
            # Check for specific fact patterns
            number_pattern = r'\b\d+(\.\d+)?\b'
            date_pattern = r'\b\d{4}\b|\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b'
            name_pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'  # Simple proper name detection
            
            factual_indicators["contains_numbers"] = bool(re.search(number_pattern, response_text))
            factual_indicators["contains_dates"] = bool(re.search(date_pattern, response_text))
            factual_indicators["contains_names"] = bool(re.search(name_pattern, response_text))
            
            # Check for specific factual content
            factual_keywords = [
                "according to", "studies show", "research indicates", "data suggests",
                "evidence shows", "statistics", "findings", "published", "peer-reviewed"
            ]
            
            factual_keyword_count = sum(1 for keyword in factual_keywords if keyword in text_lower)
            factual_indicators["contains_specific_facts"] = factual_keyword_count > 0
            
            # Calculate factual density
            total_words = len(response_text.split())
            factual_indicators["factual_density"] = factual_keyword_count / max(1, total_words) * 100
            
            # Calculate overall factual accuracy score (heuristic)
            factual_score = 0.0
            if factual_indicators["contains_numbers"]:
                factual_score += 0.3
            if factual_indicators["contains_dates"]:
                factual_score += 0.2
            if factual_indicators["contains_names"]:
                factual_score += 0.2
            if factual_indicators["contains_specific_facts"]:
                factual_score += 0.3
            
            factual_indicators["factual_accuracy_score"] = min(factual_score, 1.0)
            
        except Exception as e:
            logger.warning(f"Factual indicators analysis failed: {e}")
            factual_indicators["error"] = str(e)
        
        return factual_indicators
    
    def _analyze_response_confidence_calibration(self, response_text: str) -> Dict[str, Any]:
        """Analyze confidence calibration in the response"""
        confidence_markers = {
            'high': ["certainly", "definitely", "clearly", "obviously", "undoubtedly", "absolutely"],
            'medium': ["likely", "probably", "generally", "typically", "usually"],
            'low': ["perhaps", "maybe", "possibly", "might", "could be", "i think"],
            'uncertain': ["unsure", "not sure", "don't know", "unclear", "uncertain"]
        }
        
        text_lower = response_text.lower()
        marker_counts = {}
        total_markers = 0
        
        for level, markers in confidence_markers.items():
            count = sum(1 for marker in markers if marker in text_lower)
            marker_counts[level] = count
            total_markers += count
        
        if total_markers == 0:
            return {
                "calibration_score": 0.5,
                "confidence_distribution": {"neutral": 1.0},
                "total_markers": 0,
                "assessment": "No explicit confidence markers found"
            }
        
        # Calculate confidence distribution
        distribution = {level: count / total_markers for level, count in marker_counts.items()}
        
        # Calculate calibration score (balanced confidence is better)
        high_conf_ratio = distribution.get('high', 0)
        uncertain_ratio = distribution.get('uncertain', 0)
        
        if high_conf_ratio > 0.7:  # Very high confidence
            calibration_score = 0.4  # May be overconfident
        elif uncertain_ratio > 0.7:  # Very uncertain
            calibration_score = 0.3  # May be underconfident
        else:
            calibration_score = 0.7  # Balanced confidence
        
        return {
            "calibration_score": calibration_score,
            "confidence_distribution": distribution,
            "marker_counts": marker_counts,
            "total_markers": total_markers,
            "assessment": self._assess_confidence_calibration_quality(calibration_score)
        }
    
    def _estimate_knowledge_consistency(self, response_text: str) -> float:
        """Estimate knowledge consistency for a single response"""
        text_lower = response_text.lower()
        
        # Check for knowledge coherence indicators
        coherence_indicators = [
            "furthermore", "additionally", "moreover", "in addition",
            "consequently", "therefore", "thus", "hence",
            "however", "nevertheless", "on the other hand",
            "similarly", "likewise", "in contrast"
        ]
        
        coherence_count = sum(1 for indicator in coherence_indicators if indicator in text_lower)
        total_sentences = len([s for s in response_text.split('.') if s.strip()])
        
        if total_sentences <= 1:
            return 0.5
        
        # Higher coherence indicator density suggests better knowledge integration
        coherence_density = coherence_count / max(1, total_sentences)
        consistency_score = min(0.5 + coherence_density * 2, 1.0)  # Scale to 0.5-1.0
        
        return float(consistency_score)
    
    def _assess_knowledge_validation_results(self, validation_results: Dict[str, Any]) -> str:
        """Provide qualitative assessment of knowledge validation results"""
        factual_acc = validation_results.get("factual_accuracy", 0)
        confidence_cal = validation_results.get("confidence_calibration", 0)
        knowledge_cons = validation_results.get("knowledge_consistency", 0)
        
        avg_score = (factual_acc + confidence_cal + knowledge_cons) / 3
        
        if avg_score >= 0.8:
            return "Excellent - High factual accuracy and well-calibrated confidence"
        elif avg_score >= 0.6:
            return "Good - Generally accurate with reasonable confidence calibration"
        elif avg_score >= 0.4:
            return "Moderate - Some accuracy issues or confidence miscalibration"
        else:
            return "Poor - Significant concerns with factual accuracy or confidence"
    
    def _assess_confidence_calibration_quality(self, score: float) -> str:
        """Assess confidence calibration quality"""
        if score >= 0.7:
            return "Well-calibrated confidence markers"
        elif score >= 0.5:
            return "Reasonably calibrated confidence"
        elif score >= 0.3:
            return "Poorly calibrated confidence"
        else:
            return "Significantly miscalibrated confidence"


    # ==================== COGNITIVE PATTERN DETECTION METHODS ====================
    
    def _assess_task_understanding(self, response_text: str, test_name: str, test_category: str) -> float:
        """Assess if model understood the task requirements (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Basic task engagement indicators
        if len(response_text.strip()) < 10:
            return 0.1  # Minimal engagement
            
        # Look for task-specific elements based on test name/category
        if "haiku" in test_name.lower():
            # For haiku tasks, check for poetry structure awareness
            lines = response_text.split('\n')
            if any('syllable' in line.lower() for line in lines):
                score += 0.4
            if len([l for l in lines if l.strip()]) >= 3:
                score += 0.3
        elif "math" in test_category.lower() if test_category else False:
            # For math tasks, check for numerical reasoning
            if any(char.isdigit() for char in response_text):
                score += 0.3
            if any(op in response_text for op in ['+', '-', '×', '*', '÷', '/', '=']):
                score += 0.3
        elif "cultural" in test_category.lower() if test_category else False:
            # For cultural tasks, check for cultural awareness
            if any(word in response_lower for word in ['tradition', 'culture', 'heritage', 'custom']):
                score += 0.4
                
        # General task understanding indicators
        if "step" in response_lower or "first" in response_lower:
            score += 0.2  # Shows structured approach
        if "because" in response_lower or "therefore" in response_lower:
            score += 0.2  # Shows reasoning
            
        return min(score, 1.0)
    
    def _assess_instruction_following(self, response_text: str, test_name: str, test_category: str) -> float:
        """Assess adherence to specific instructions/format (0-1)"""
        score = 0.0
        
        # Check for explicit instruction following
        if "complete" in test_name.lower() or "finish" in test_name.lower():
            # Should provide completion
            if len(response_text.split()) > 5:
                score += 0.5
        
        # Format following indicators
        response_lines = response_text.split('\n')
        if len(response_lines) > 1:
            score += 0.3  # Multi-line suggests structure awareness
            
        # Look for enumeration if applicable
        if any(line.strip().startswith(('1.', '2.', '3.', '-', '•')) for line in response_lines):
            score += 0.4  # Structured format
            
        return min(score, 1.0)
    
    def _assess_context_awareness(self, response_text: str, test_name: str, test_category: str) -> float:
        """Assess domain knowledge and contextual understanding (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Domain-specific context awareness
        if test_category and "reasoning" in test_category.lower():
            reasoning_indicators = ['analyze', 'consider', 'conclude', 'infer', 'deduce']
            score += 0.2 * sum(1 for indicator in reasoning_indicators if indicator in response_lower)
            
        if test_category and "cultural" in test_category.lower():
            cultural_indicators = ['respect', 'tradition', 'heritage', 'authentic', 'significance']  
            score += 0.15 * sum(1 for indicator in cultural_indicators if indicator in response_lower)
            
        # General context indicators
        if len(response_text.split()) > 50:  # Substantial response suggests context awareness
            score += 0.3
            
        return min(score, 1.0)
    
    def _assess_logical_structure(self, response_text: str, word_count: int) -> float:
        """Assess logical flow and structure (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Structured reasoning indicators
        structure_words = ['first', 'second', 'then', 'next', 'finally', 'therefore', 'because', 'since']
        structure_count = sum(1 for word in structure_words if word in response_lower)
        score += min(structure_count * 0.15, 0.6)
        
        # Logical flow indicators
        if 'therefore' in response_lower or 'thus' in response_lower:
            score += 0.2
        if 'because' in response_lower or 'since' in response_lower:
            score += 0.2
            
        return min(score, 1.0)
    
    def _assess_evidence_integration(self, response_text: str, word_count: int) -> float:
        """Assess use of relevant information (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Evidence indicators
        evidence_words = ['based on', 'according to', 'evidence', 'shows', 'indicates', 'suggests']
        evidence_count = sum(1 for phrase in evidence_words if phrase in response_lower)
        score += min(evidence_count * 0.2, 0.6)
        
        # Information integration indicators
        if word_count > 30:  # Substantial content suggests information use
            score += 0.2
        if word_count > 100:  # Extensive content suggests thorough integration
            score += 0.2
            
        return min(score, 1.0)
    
    def _assess_inference_quality(self, response_text: str, reasoning_type: Optional[Union[str, ReasoningType]]) -> float:
        """Assess logical conclusion quality (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Inference indicators
        inference_words = ['conclude', 'infer', 'deduce', 'implies', 'suggests', 'indicates']
        score += min(sum(1 for word in inference_words if word in response_lower) * 0.2, 0.6)
        
        # Chain of thought specific
        if reasoning_type and 'chain' in str(reasoning_type).lower():
            if 'step' in response_lower and ('1' in response_text or 'first' in response_lower):
                score += 0.3
                
        return min(score, 1.0)
    
    def _assess_mathematical_reasoning(self, response_text: str, test_category: str) -> float:
        """Assess mathematical reasoning capability (0-1)"""
        if not test_category or 'math' not in test_category.lower():
            return 0.0  # Only applicable to math domains
            
        score = 0.0
        
        # Mathematical content indicators
        if any(char.isdigit() for char in response_text):
            score += 0.3
        if any(op in response_text for op in ['+', '-', '×', '*', '÷', '/', '=', '<', '>']):
            score += 0.3
        if any(word in response_text.lower() for word in ['calculate', 'equation', 'formula', 'solve']):
            score += 0.4
            
        return min(score, 1.0)
    
    def _assess_cultural_sensitivity(self, response_text: str, test_category: str) -> float:
        """Assess cultural awareness and sensitivity (0-1)"""
        if not test_category or 'cultural' not in test_category.lower():
            return 0.0  # Only applicable to cultural domains
            
        score = 0.0
        response_lower = response_text.lower()
        
        # Cultural sensitivity indicators
        sensitive_words = ['respect', 'tradition', 'heritage', 'culture', 'authentic', 'honor']
        score += min(sum(1 for word in sensitive_words if word in response_lower) * 0.15, 0.6)
        
        # Avoid stereotyping language
        if not any(problematic in response_lower for problematic in ['always', 'never', 'all people', 'they all']):
            score += 0.4
            
        return min(score, 1.0)
    
    def _assess_creative_synthesis(self, response_text: str, test_category: str) -> float:
        """Assess creative and original thinking (0-1)"""
        if not test_category or 'creative' not in test_category.lower():
            return 0.0  # Only applicable to creative domains
            
        score = 0.0
        response_lower = response_text.lower()
        
        # Creative indicators
        creative_words = ['imagine', 'creative', 'original', 'unique', 'innovative', 'novel']
        score += min(sum(1 for word in creative_words if word in response_lower) * 0.2, 0.6)
        
        # Length as creativity indicator (more elaborate = more creative)
        word_count = len(response_text.split())
        if word_count > 100:
            score += 0.4
            
        return min(score, 1.0)
    
    def _assess_analytical_decomposition(self, response_text: str, test_category: str) -> float:
        """Assess systematic problem breakdown (0-1)"""
        if not test_category or 'analytical' not in test_category.lower():
            return 0.0  # Only applicable to analytical domains
            
        score = 0.0
        response_lower = response_text.lower()
        
        # Analytical indicators
        analytical_words = ['analyze', 'break down', 'component', 'element', 'factor', 'aspect']
        score += min(sum(1 for word in analytical_words if word in response_lower) * 0.15, 0.6)
        
        # Structured decomposition
        if any(response_text.count(str(i)) > 0 for i in range(1, 4)):  # Numbered points
            score += 0.4
            
        return min(score, 1.0)
    
    def _assess_relevance(self, response_text: str, test_name: str, word_count: int) -> float:
        """Assess how on-topic and focused the response is (0-1)"""
        score = 0.5  # Base relevance score
        
        # Length-based relevance (too short = unfocused, too long = potentially off-topic)
        if word_count < 5:
            score = 0.1  # Too brief to be relevant
        elif word_count > 500:
            score = max(score - 0.2, 0.3)  # Potentially unfocused if too long
        elif 10 <= word_count <= 200:
            score += 0.3  # Good length for focused response
            
        # Basic coherence check (no repeated words/phrases indicating confusion)
        words = response_text.lower().split()
        if words and len(set(words)) / len(words) > 0.7:  # High vocabulary diversity
            score += 0.2
            
        return min(score, 1.0)
    
    def _assess_depth(self, response_text: str, word_count: int) -> float:
        """Assess thoroughness of concept exploration (0-1)"""
        score = 0.0
        response_lower = response_text.lower()
        
        # Depth indicators
        depth_words = ['detailed', 'comprehensive', 'thorough', 'extensive', 'in-depth', 'elaborate']
        score += min(sum(1 for word in depth_words if word in response_lower) * 0.2, 0.4)
        
        # Length as depth indicator
        if word_count > 50:
            score += 0.3
        if word_count > 150:
            score += 0.3
            
        return min(score, 1.0)
    
    def _assess_cognitive_coherence(self, response_text: str, word_count: int) -> float:
        """Assess internal consistency and flow for cognitive pattern detection (0-1)"""
        score = 0.5  # Base coherence
        
        # Coherence indicators
        coherence_words = ['therefore', 'however', 'furthermore', 'moreover', 'additionally']
        score += min(sum(1 for word in coherence_words if word in response_text.lower()) * 0.1, 0.3)
        
        # Structural coherence (sentences, paragraphs)
        sentences = response_text.split('.')
        if len(sentences) > 2:
            score += 0.2  # Multi-sentence suggests structured thought
            
        return min(score, 1.0)


# Convenience function for quick evaluation
def evaluate_reasoning(response_text: str, test_name: str, test_category: Optional[str] = None, **kwargs) -> EvaluationResult:
    """
    Convenience function for universal evaluation
    
    Args:
        response_text: Text to evaluate
        test_name: Name of the test
        test_category: Test category for type-specific evaluation
        **kwargs: Additional arguments for UniversalEvaluator
    
    Returns:
        EvaluationResult
    """
    evaluator = UniversalEvaluator()
    return evaluator.evaluate_response(response_text, test_name, test_category=test_category, **kwargs)