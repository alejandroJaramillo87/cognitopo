"""
Pattern-Based Evaluator

Implements the key insight: Focus on PATTERN RECOGNITION rather than absolute truth.
This evaluator analyzes consistent behavioral patterns, response consistency, 
and comparative model performance rather than judging "correctness."

Key Philosophy:
- Pattern detection over truth validation
- Behavioral consistency analysis  
- Multi-model comparative scoring
- Scalable across domains without golden references

Built on proven calibration framework with 75% success rate.
"""

import re
import json
import logging
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import Counter
import statistics

logger = logging.getLogger(__name__)

@dataclass
class PatternAnalysisResult:
    """Results from pattern-based analysis"""
    response_consistency: float        # How consistent are responses to similar prompts
    behavioral_signature: Dict[str, Any]  # Model's distinctive response patterns
    pattern_adherence: float          # How well does response follow expected patterns
    comparative_ranking: Optional[float]  # Ranking vs other models (if available)
    quality_indicators: Dict[str, float]  # Various quality metrics
    
@dataclass 
class ResponsePatterns:
    """Detected patterns in model responses"""
    length_patterns: Dict[str, Any]
    structural_patterns: Dict[str, Any]
    content_patterns: Dict[str, Any]
    consistency_patterns: Dict[str, Any]

class PatternBasedEvaluator:
    """
    Evaluator focused on pattern recognition and behavioral analysis
    
    Instead of asking "Is this response correct?", asks:
    - "What patterns does this model exhibit?"
    - "How consistent is this model's behavior?" 
    - "What are this model's distinctive characteristics?"
    """
    
    def __init__(self):
        # Pattern detection settings
        self.min_pattern_confidence = 0.7
        self.consistency_threshold = 0.8
        
        # Response pattern storage for comparative analysis
        self.response_history = []
        self.behavioral_baselines = {}
        
    def evaluate_patterns(self, 
                         response_text: str,
                         prompt: str,
                         test_metadata: Dict[str, Any],
                         model_id: str = "current") -> PatternAnalysisResult:
        """
        Main pattern evaluation method
        
        Args:
            response_text: The model's response
            prompt: Original prompt  
            test_metadata: Test configuration and expected patterns
            model_id: Identifier for the model being tested
            
        Returns:
            PatternAnalysisResult with comprehensive pattern analysis
        """
        
        logger.debug(f"Analyzing patterns for {model_id}")
        
        # Detect response patterns
        response_patterns = self._detect_response_patterns(response_text, prompt)
        
        # Analyze behavioral consistency
        consistency_score = self._analyze_consistency(response_patterns, model_id)
        
        # Generate behavioral signature
        behavioral_signature = self._generate_behavioral_signature(response_patterns)
        
        # Check pattern adherence to expected patterns (if provided)
        pattern_adherence = self._check_pattern_adherence(
            response_patterns, test_metadata.get('expected_patterns', [])
        )
        
        # Calculate quality indicators
        quality_indicators = self._calculate_quality_indicators(
            response_text, response_patterns
        )
        
        # Store for future comparative analysis
        self._store_response_data(response_text, response_patterns, model_id)
        
        # Comparative ranking (if multiple models tested)
        comparative_ranking = self._calculate_comparative_ranking(
            behavioral_signature, model_id
        )
        
        return PatternAnalysisResult(
            response_consistency=consistency_score,
            behavioral_signature=behavioral_signature,
            pattern_adherence=pattern_adherence,
            comparative_ranking=comparative_ranking,
            quality_indicators=quality_indicators
        )
    
    def _detect_response_patterns(self, response: str, prompt: str) -> ResponsePatterns:
        """Detect various patterns in the response"""
        
        # Length patterns
        length_patterns = {
            'total_length': len(response),
            'word_count': len(response.split()),
            'sentence_count': len([s for s in response.split('.') if s.strip()]),
            'paragraph_count': len([p for p in response.split('\n\n') if p.strip()]),
            'avg_sentence_length': len(response.split()) / max(1, len([s for s in response.split('.') if s.strip()]))
        }
        
        # Structural patterns
        structural_patterns = {
            'starts_with_capital': response.strip().startswith(response.strip()[0].upper()) if response.strip() else False,
            'ends_with_punctuation': response.strip().endswith(('.', '!', '?')) if response.strip() else False,
            'contains_lists': bool(re.search(r'[1-9]\.|â€¢|\*|\-\s', response)),
            'contains_quotes': response.count('"') + response.count("'"),
            'contains_questions': response.count('?'),
            'contains_emphasis': response.count('!'),
            'paragraph_structure': 'single' if '\n\n' not in response else 'multi'
        }
        
        # Content patterns  
        content_patterns = {
            'technical_terms': len(re.findall(r'\b[A-Z]{2,}|\b\w*(?:tion|sion|ness|ment)\b', response)),
            'numbers_mentioned': len(re.findall(r'\b\d+\b', response)),
            'proper_nouns': len(re.findall(r'\b[A-Z][a-z]+\b', response)),
            'conjunctions': len(re.findall(r'\b(?:and|but|or|however|therefore|moreover|furthermore)\b', response, re.IGNORECASE)),
            'hedge_words': len(re.findall(r'\b(?:perhaps|maybe|possibly|likely|might|could|seems|appears)\b', response, re.IGNORECASE)),
            'certainty_words': len(re.findall(r'\b(?:definitely|certainly|absolutely|clearly|obviously|always|never)\b', response, re.IGNORECASE))
        }
        
        # Consistency patterns (repetition detection)
        words = response.lower().split()
        word_freq = Counter(words)
        sentences = [s.strip() for s in response.split('.') if len(s.strip()) > 10]
        sentence_freq = Counter(sentences)
        
        consistency_patterns = {
            'word_repetition_rate': sum(1 for count in word_freq.values() if count > 3) / max(1, len(word_freq)),
            'sentence_repetition_count': sum(1 for count in sentence_freq.values() if count > 1),
            'max_word_repetition': max(word_freq.values()) if word_freq else 0,
            'vocabulary_diversity': len(set(words)) / max(1, len(words)),
            'repetitive_loops': sum(1 for count in sentence_freq.values() if count > 2)  # Our key metric
        }
        
        return ResponsePatterns(
            length_patterns=length_patterns,
            structural_patterns=structural_patterns, 
            content_patterns=content_patterns,
            consistency_patterns=consistency_patterns
        )
    
    def _analyze_consistency(self, patterns: ResponsePatterns, model_id: str) -> float:
        """Analyze behavioral consistency compared to model's history"""
        
        if model_id not in self.behavioral_baselines:
            # First response from this model - establish baseline
            return 1.0
            
        baseline = self.behavioral_baselines[model_id]
        
        # Compare current patterns to established baseline
        consistency_scores = []
        
        # Length consistency
        length_consistency = self._calculate_pattern_consistency(
            patterns.length_patterns, baseline.get('length_patterns', {})
        )
        consistency_scores.append(length_consistency)
        
        # Structural consistency  
        structural_consistency = self._calculate_pattern_consistency(
            patterns.structural_patterns, baseline.get('structural_patterns', {})
        )
        consistency_scores.append(structural_consistency)
        
        # Content consistency
        content_consistency = self._calculate_pattern_consistency(
            patterns.content_patterns, baseline.get('content_patterns', {})
        )
        consistency_scores.append(content_consistency)
        
        return statistics.mean(consistency_scores) if consistency_scores else 0.5
    
    def _calculate_pattern_consistency(self, current: Dict, baseline: Dict) -> float:
        """Calculate consistency between current and baseline patterns"""
        if not baseline:
            return 1.0
            
        similarities = []
        for key in current.keys():
            if key in baseline:
                current_val = current[key]
                baseline_val = baseline[key]
                
                if isinstance(current_val, (int, float)) and isinstance(baseline_val, (int, float)):
                    # Numerical similarity
                    if baseline_val == 0:
                        similarity = 1.0 if current_val == 0 else 0.0
                    else:
                        diff_ratio = abs(current_val - baseline_val) / max(abs(baseline_val), 1)
                        similarity = max(0, 1.0 - diff_ratio)
                    similarities.append(similarity)
                elif isinstance(current_val, bool) and isinstance(baseline_val, bool):
                    # Boolean similarity
                    similarities.append(1.0 if current_val == baseline_val else 0.0)
                elif isinstance(current_val, str) and isinstance(baseline_val, str):
                    # String similarity
                    similarities.append(1.0 if current_val == baseline_val else 0.0)
                    
        return statistics.mean(similarities) if similarities else 0.5
    
    def _generate_behavioral_signature(self, patterns: ResponsePatterns) -> Dict[str, Any]:
        """Generate a distinctive behavioral signature for the model"""
        
        signature = {
            # Response style indicators
            'response_style': self._classify_response_style(patterns),
            'verbosity_level': self._classify_verbosity(patterns),
            'formality_level': self._classify_formality(patterns),
            
            # Behavioral tendencies
            'repetition_tendency': patterns.consistency_patterns['repetitive_loops'],
            'elaboration_tendency': patterns.length_patterns['avg_sentence_length'],
            'uncertainty_tendency': patterns.content_patterns['hedge_words'],
            'assertiveness_tendency': patterns.content_patterns['certainty_words'],
            
            # Structural preferences
            'uses_lists': patterns.structural_patterns['contains_lists'],
            'uses_questions': patterns.structural_patterns['contains_questions'] > 0,
            'paragraph_preference': patterns.structural_patterns['paragraph_structure'],
            
            # Content characteristics
            'technical_inclination': patterns.content_patterns['technical_terms'] / max(1, patterns.length_patterns['word_count']),
            'vocabulary_richness': patterns.consistency_patterns['vocabulary_diversity'],
        }
        
        return signature
    
    def _classify_response_style(self, patterns: ResponsePatterns) -> str:
        """Classify the model's response style"""
        
        if patterns.content_patterns['technical_terms'] > patterns.length_patterns['word_count'] * 0.1:
            return 'technical'
        elif patterns.content_patterns['hedge_words'] > 3:
            return 'cautious'
        elif patterns.content_patterns['certainty_words'] > 2:
            return 'assertive'
        elif patterns.structural_patterns['contains_questions'] > 2:
            return 'inquisitive'
        else:
            return 'balanced'
    
    def _classify_verbosity(self, patterns: ResponsePatterns) -> str:
        """Classify verbosity level"""
        word_count = patterns.length_patterns['word_count']
        
        if word_count < 50:
            return 'concise'
        elif word_count < 200:
            return 'moderate' 
        elif word_count < 400:
            return 'verbose'
        else:
            return 'very_verbose'
    
    def _classify_formality(self, patterns: ResponsePatterns) -> str:
        """Classify formality level"""
        
        formal_indicators = (
            patterns.content_patterns['technical_terms'] +
            patterns.content_patterns['conjunctions'] +
            (1 if patterns.structural_patterns['paragraph_structure'] == 'multi' else 0)
        )
        
        informal_indicators = (
            patterns.structural_patterns['contains_emphasis'] +
            patterns.content_patterns['hedge_words']
        )
        
        if formal_indicators > informal_indicators * 2:
            return 'formal'
        elif informal_indicators > formal_indicators * 2:
            return 'informal'
        else:
            return 'mixed'
    
    def _check_pattern_adherence(self, patterns: ResponsePatterns, expected_patterns: List[str]) -> float:
        """Check how well response adheres to expected patterns"""
        
        if not expected_patterns:
            return 0.8  # Neutral score when no patterns specified
            
        # This is where we apply the pattern recognition approach
        # Instead of exact matching, look for behavioral consistency with expected patterns
        
        adherence_indicators = []
        
        # Check if response exhibits patterns consistent with expectations
        for expected in expected_patterns:
            expected_lower = expected.lower()
            
            # Pattern matching heuristics (not exact truth validation)
            if any(word in expected_lower for word in ['brief', 'short', 'concise']):
                # Expected brevity pattern
                adherence = 1.0 if patterns.length_patterns['word_count'] < 100 else 0.5
                adherence_indicators.append(adherence)
                
            elif any(word in expected_lower for word in ['detailed', 'comprehensive', 'thorough']):
                # Expected detail pattern  
                adherence = 1.0 if patterns.length_patterns['word_count'] > 200 else 0.5
                adherence_indicators.append(adherence)
                
            elif any(word in expected_lower for word in ['question', 'ask', 'inquiry']):
                # Expected questioning pattern
                adherence = 1.0 if patterns.structural_patterns['contains_questions'] > 0 else 0.3
                adherence_indicators.append(adherence)
                
            elif any(word in expected_lower for word in ['list', 'enumerate', 'steps']):
                # Expected listing pattern
                adherence = 1.0 if patterns.structural_patterns['contains_lists'] else 0.3
                adherence_indicators.append(adherence)
                
            else:
                # General pattern adherence - focus on response appropriateness
                adherence = 0.7  # Neutral for unspecified patterns
                adherence_indicators.append(adherence)
        
        return statistics.mean(adherence_indicators) if adherence_indicators else 0.7
    
    def _calculate_quality_indicators(self, response: str, patterns: ResponsePatterns) -> Dict[str, float]:
        """Calculate various quality indicators"""
        
        return {
            'coherence_score': self._calculate_coherence_score(response, patterns),
            'completeness_score': self._calculate_completeness_score(patterns),
            'fluency_score': self._calculate_fluency_score(patterns),
            'engagement_score': self._calculate_engagement_score(patterns),
            'loop_penalty': max(0, 1.0 - (patterns.consistency_patterns['repetitive_loops'] * 0.2))
        }
    
    def _calculate_coherence_score(self, response: str, patterns: ResponsePatterns) -> float:
        """Calculate response coherence"""
        
        # Coherence indicators
        coherence_score = 1.0
        
        # Penalize excessive repetition (our key insight from calibration)
        repetition_penalty = min(0.5, patterns.consistency_patterns['repetitive_loops'] * 0.1)
        coherence_score -= repetition_penalty
        
        # Reward vocabulary diversity
        diversity_bonus = patterns.consistency_patterns['vocabulary_diversity'] * 0.2
        coherence_score += diversity_bonus
        
        # Penalize very short responses that might be incomplete
        if patterns.length_patterns['word_count'] < 20:
            coherence_score *= 0.7
            
        return max(0, min(1.0, coherence_score))
    
    def _calculate_completeness_score(self, patterns: ResponsePatterns) -> float:
        """Calculate response completeness"""
        
        # Completeness based on response development
        completeness = 0.5  # Base score
        
        # Length indicators
        if patterns.length_patterns['word_count'] > 50:
            completeness += 0.2
        if patterns.length_patterns['word_count'] > 150:
            completeness += 0.2
            
        # Structure indicators  
        if patterns.structural_patterns['paragraph_structure'] == 'multi':
            completeness += 0.1
            
        return min(1.0, completeness)
    
    def _calculate_fluency_score(self, patterns: ResponsePatterns) -> float:
        """Calculate response fluency"""
        
        fluency = 0.8  # Base fluency
        
        # Sentence structure
        avg_sentence_length = patterns.length_patterns['avg_sentence_length']
        if 10 < avg_sentence_length < 25:  # Optimal range
            fluency += 0.1
        elif avg_sentence_length > 35:  # Too long
            fluency -= 0.1
            
        # Proper capitalization and punctuation
        if patterns.structural_patterns['starts_with_capital']:
            fluency += 0.05
        if patterns.structural_patterns['ends_with_punctuation']:
            fluency += 0.05
            
        return min(1.0, max(0, fluency))
    
    def _calculate_engagement_score(self, patterns: ResponsePatterns) -> float:
        """Calculate response engagement"""
        
        engagement = 0.5  # Base engagement
        
        # Variety indicators
        if patterns.structural_patterns['contains_questions'] > 0:
            engagement += 0.15
        if patterns.structural_patterns['contains_lists']:
            engagement += 0.1
        if patterns.content_patterns['proper_nouns'] > 0:
            engagement += 0.1
            
        # Balanced certainty/uncertainty
        hedge_ratio = patterns.content_patterns['hedge_words'] / max(1, patterns.length_patterns['word_count'])
        if 0.01 < hedge_ratio < 0.05:  # Balanced uncertainty
            engagement += 0.15
            
        return min(1.0, engagement)
    
    def _store_response_data(self, response: str, patterns: ResponsePatterns, model_id: str):
        """Store response data for future pattern analysis"""
        
        # Update behavioral baselines
        if model_id not in self.behavioral_baselines:
            self.behavioral_baselines[model_id] = {}
            
        baseline = self.behavioral_baselines[model_id]
        
        # Update running averages for patterns
        for pattern_type in ['length_patterns', 'structural_patterns', 'content_patterns', 'consistency_patterns']:
            pattern_data = getattr(patterns, pattern_type)
            
            if pattern_type not in baseline:
                baseline[pattern_type] = pattern_data.copy()
            else:
                # Update running averages
                for key, value in pattern_data.items():
                    if isinstance(value, (int, float)):
                        if key in baseline[pattern_type]:
                            baseline[pattern_type][key] = (baseline[pattern_type][key] + value) / 2
                        else:
                            baseline[pattern_type][key] = value
                            
        # Store response history
        self.response_history.append({
            'model_id': model_id,
            'response': response[:200],  # Store first 200 chars
            'patterns': patterns,
            'timestamp': len(self.response_history)
        })
    
    def _calculate_comparative_ranking(self, signature: Dict[str, Any], model_id: str) -> Optional[float]:
        """Calculate ranking compared to other models (if available)"""
        
        # This would compare against other models if available
        # For now, return None to indicate single-model testing
        return None
    
    def generate_pattern_summary(self, result: PatternAnalysisResult, model_id: str = "current") -> Dict[str, Any]:
        """Generate human-readable summary of pattern analysis"""
        
        return {
            'model_id': model_id,
            'overall_consistency': result.response_consistency,
            'pattern_adherence': result.pattern_adherence,
            'behavioral_profile': {
                'style': result.behavioral_signature.get('response_style', 'unknown'),
                'verbosity': result.behavioral_signature.get('verbosity_level', 'unknown'),
                'formality': result.behavioral_signature.get('formality_level', 'unknown')
            },
            'quality_assessment': {
                'coherence': result.quality_indicators.get('coherence_score', 0),
                'completeness': result.quality_indicators.get('completeness_score', 0),
                'engagement': result.quality_indicators.get('engagement_score', 0)
            },
            'distinctive_traits': [
                trait for trait, value in result.behavioral_signature.items()
                if isinstance(value, (int, float)) and value > 0.7
            ]
        }

# Integration with existing evaluator framework
def create_pattern_based_evaluation(response: str, 
                                   test_data: Dict[str, Any], 
                                   model_id: str = "current") -> Dict[str, Any]:
    """
    Create a pattern-based evaluation result compatible with existing framework
    
    This bridges the pattern-focused approach with the existing evaluation system
    """
    
    evaluator = PatternBasedEvaluator()
    
    pattern_result = evaluator.evaluate_patterns(
        response_text=response,
        prompt=test_data.get('prompt', ''),
        test_metadata=test_data,
        model_id=model_id
    )
    
    # Convert to compatible format
    return {
        'pattern_analysis': asdict(pattern_result),
        'summary': evaluator.generate_pattern_summary(pattern_result, model_id),
        'approach': 'pattern_recognition',
        'focus': 'behavioral_consistency_over_absolute_truth'
    }